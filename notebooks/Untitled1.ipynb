{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fb53dd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\" Time Series Transformer model configuration\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     40\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n",
    "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" PyTorch Time Series Transformer model.\"\"\"\n",
    "\n",
    "# coding=utf-8\n",
    "# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Time Series Transformer model configuration\"\"\"\n",
    "\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from ...configuration_utils import PretrainedConfig\n",
    "from ...utils import logging\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from ...activations import ACT2FN\n",
    "from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n",
    "from ...modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    SampleTSPredictionOutput,\n",
    "    Seq2SeqTSModelOutput,\n",
    "    Seq2SeqTSPredictionOutput,\n",
    ")\n",
    "from ...modeling_utils import PreTrainedModel\n",
    "from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n",
    "from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
    "from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "TIME_SERIES_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"huggingface/time-series-transformer-tourism-monthly\": (\n",
    "        \"https://huggingface.co/huggingface/time-series-transformer-tourism-monthly/resolve/main/config.json\"\n",
    "    ),\n",
    "    # See all TimeSeriesTransformer models at https://huggingface.co/models?filter=time_series_transformer\n",
    "}\n",
    "\n",
    "\n",
    "class TimeSeriesTransformerConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`TimeSeriesTransformerModel`]. It is used to\n",
    "    instantiate a Time Series Transformer model according to the specified arguments, defining the model architecture.\n",
    "    Instantiating a configuration with the defaults will yield a similar configuration to that of the Time Series\n",
    "    Transformer\n",
    "    [huggingface/time-series-transformer-tourism-monthly](https://huggingface.co/huggingface/time-series-transformer-tourism-monthly)\n",
    "    architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "    Args:\n",
    "        prediction_length (`int`):\n",
    "            The prediction length for the decoder. In other words, the prediction horizon of the model. This value is\n",
    "            typically dictated by the dataset and we recommend to set it appropriately.\n",
    "        context_length (`int`, *optional*, defaults to `prediction_length`):\n",
    "            The context length for the encoder. If `None`, the context length will be the same as the\n",
    "            `prediction_length`.\n",
    "        distribution_output (`string`, *optional*, defaults to `\"student_t\"`):\n",
    "            The distribution emission head for the model. Could be either \"student_t\", \"normal\" or \"negative_binomial\".\n",
    "        loss (`string`, *optional*, defaults to `\"nll\"`):\n",
    "            The loss function for the model corresponding to the `distribution_output` head. For parametric\n",
    "            distributions it is the negative log likelihood (nll) - which currently is the only supported one.\n",
    "        input_size (`int`, *optional*, defaults to 1):\n",
    "            The size of the target variable which by default is 1 for univariate targets. Would be > 1 in case of\n",
    "            multivariate targets.\n",
    "        scaling (`string` or `bool`, *optional* defaults to `\"mean\"`):\n",
    "            Whether to scale the input targets via \"mean\" scaler, \"std\" scaler or no scaler if `None`. If `True`, the\n",
    "            scaler is set to \"mean\".\n",
    "        lags_sequence (`list[int]`, *optional*, defaults to `[1, 2, 3, 4, 5, 6, 7]`):\n",
    "            The lags of the input time series as covariates often dictated by the frequency of the data. Default is\n",
    "            `[1, 2, 3, 4, 5, 6, 7]` but we recommend to change it based on the dataset appropriately.\n",
    "        num_time_features (`int`, *optional*, defaults to 0):\n",
    "            The number of time features in the input time series.\n",
    "        num_dynamic_real_features (`int`, *optional*, defaults to 0):\n",
    "            The number of dynamic real valued features.\n",
    "        num_static_categorical_features (`int`, *optional*, defaults to 0):\n",
    "            The number of static categorical features.\n",
    "        num_static_real_features (`int`, *optional*, defaults to 0):\n",
    "            The number of static real valued features.\n",
    "        cardinality (`list[int]`, *optional*):\n",
    "            The cardinality (number of different values) for each of the static categorical features. Should be a list\n",
    "            of integers, having the same length as `num_static_categorical_features`. Cannot be `None` if\n",
    "            `num_static_categorical_features` is > 0.\n",
    "        embedding_dimension (`list[int]`, *optional*):\n",
    "            The dimension of the embedding for each of the static categorical features. Should be a list of integers,\n",
    "            having the same length as `num_static_categorical_features`. Cannot be `None` if\n",
    "            `num_static_categorical_features` is > 0.\n",
    "        d_model (`int`, *optional*, defaults to 64):\n",
    "            Dimensionality of the transformer layers.\n",
    "        encoder_layers (`int`, *optional*, defaults to 2):\n",
    "            Number of encoder layers.\n",
    "        decoder_layers (`int`, *optional*, defaults to 2):\n",
    "            Number of decoder layers.\n",
    "        encoder_attention_heads (`int`, *optional*, defaults to 2):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        decoder_attention_heads (`int`, *optional*, defaults to 2):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder.\n",
    "        encoder_ffn_dim (`int`, *optional*, defaults to 32):\n",
    "            Dimension of the \"intermediate\" (often named feed-forward) layer in encoder.\n",
    "        decoder_ffn_dim (`int`, *optional*, defaults to 32):\n",
    "            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n",
    "        activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and decoder. If string, `\"gelu\"` and\n",
    "            `\"relu\"` are supported.\n",
    "        dropout (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability for all fully connected layers in the encoder, and decoder.\n",
    "        encoder_layerdrop (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability for the attention and fully connected layers for each encoder layer.\n",
    "        decoder_layerdrop (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability for the attention and fully connected layers for each decoder layer.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability for the attention probabilities.\n",
    "        activation_dropout (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability used between the two layers of the feed-forward networks.\n",
    "        num_parallel_samples (`int`, *optional*, defaults to 100):\n",
    "            The number of samples to generate in parallel for each time step of inference.\n",
    "        init_std (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated normal weight initialization distribution.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to use the past key/values attentions (if applicable to the model) to speed up decoding.\n",
    "\n",
    "        Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "    >>> # Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "    >>> configuration = TimeSeriesTransformerConfig(prediction_length=12)\n",
    "\n",
    "    >>> # Randomly initializing a model (with random weights) from the configuration\n",
    "    >>> model = TimeSeriesTransformerModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "    model_type = \"time_series_transformer\"\n",
    "    attribute_map = {\n",
    "        \"hidden_size\": \"d_model\",\n",
    "        \"num_attention_heads\": \"encoder_attention_heads\",\n",
    "        \"num_hidden_layers\": \"encoder_layers\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length: Optional[int] = None,\n",
    "        context_length: Optional[int] = None,\n",
    "        distribution_output: str = \"student_t\",\n",
    "        loss: str = \"nll\",\n",
    "        input_size: int = 1,\n",
    "        lags_sequence: List[int] = [1, 2, 3, 4, 5, 6, 7],\n",
    "        scaling: Optional[Union[str, bool]] = \"mean\",\n",
    "        num_dynamic_real_features: int = 0,\n",
    "        num_static_categorical_features: int = 0,\n",
    "        num_static_real_features: int = 0,\n",
    "        num_time_features: int = 0,\n",
    "        cardinality: Optional[List[int]] = None,\n",
    "        embedding_dimension: Optional[List[int]] = None,\n",
    "        encoder_ffn_dim: int = 32,\n",
    "        decoder_ffn_dim: int = 32,\n",
    "        encoder_attention_heads: int = 2,\n",
    "        decoder_attention_heads: int = 2,\n",
    "        encoder_layers: int = 2,\n",
    "        decoder_layers: int = 2,\n",
    "        is_encoder_decoder: bool = True,\n",
    "        activation_function: str = \"gelu\",\n",
    "        d_model: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        encoder_layerdrop: float = 0.1,\n",
    "        decoder_layerdrop: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        activation_dropout: float = 0.1,\n",
    "        num_parallel_samples: int = 100,\n",
    "        init_std: float = 0.02,\n",
    "        use_cache=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # time series specific configuration\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length or prediction_length\n",
    "        self.distribution_output = distribution_output\n",
    "        self.loss = loss\n",
    "        self.input_size = input_size\n",
    "        self.num_time_features = num_time_features\n",
    "        self.lags_sequence = lags_sequence\n",
    "        self.scaling = scaling\n",
    "        self.num_dynamic_real_features = num_dynamic_real_features\n",
    "        self.num_static_real_features = num_static_real_features\n",
    "        self.num_static_categorical_features = num_static_categorical_features\n",
    "        if cardinality and num_static_categorical_features > 0:\n",
    "            if len(cardinality) != num_static_categorical_features:\n",
    "                raise ValueError(\n",
    "                    \"The cardinality should be a list of the same length as `num_static_categorical_features`\"\n",
    "                )\n",
    "            self.cardinality = cardinality\n",
    "        else:\n",
    "            self.cardinality = [0]\n",
    "        if embedding_dimension and num_static_categorical_features > 0:\n",
    "            if len(embedding_dimension) != num_static_categorical_features:\n",
    "                raise ValueError(\n",
    "                    \"The embedding dimension should be a list of the same length as `num_static_categorical_features`\"\n",
    "                )\n",
    "            self.embedding_dimension = embedding_dimension\n",
    "        else:\n",
    "            self.embedding_dimension = [min(50, (cat + 1) // 2) for cat in self.cardinality]\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "\n",
    "        # Transformer architecture configuration\n",
    "        self.feature_size = input_size * len(lags_sequence) + self._number_of_features\n",
    "        self.d_model = d_model\n",
    "        self.encoder_attention_heads = encoder_attention_heads\n",
    "        self.decoder_attention_heads = decoder_attention_heads\n",
    "        self.encoder_ffn_dim = encoder_ffn_dim\n",
    "        self.decoder_ffn_dim = decoder_ffn_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.encoder_layerdrop = encoder_layerdrop\n",
    "        self.decoder_layerdrop = decoder_layerdrop\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _number_of_features(self) -> int:\n",
    "        return (\n",
    "            sum(self.embedding_dimension)\n",
    "            + self.num_dynamic_real_features\n",
    "            + self.num_time_features\n",
    "            + self.num_static_real_features\n",
    "            + self.input_size * 2  # the log1p(abs(loc)) and log(scale) features\n",
    "        )\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"TimeSeriesTransformerConfig\"\n",
    "\n",
    "\n",
    "TIME_SERIES_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"huggingface/time-series-transformer-tourism-monthly\",\n",
    "    # See all TimeSeriesTransformer models at https://huggingface.co/models?filter=time_series_transformer\n",
    "]\n",
    "\n",
    "\n",
    "class TimeSeriesFeatureEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed a sequence of categorical features.\n",
    "\n",
    "    Args:\n",
    "        cardinalities (`list[int]`):\n",
    "            List of cardinalities of the categorical features.\n",
    "        embedding_dims (`list[int]`):\n",
    "            List of embedding dimensions of the categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = len(cardinalities)\n",
    "        self.embedders = nn.ModuleList([nn.Embedding(c, d) for c, d in zip(cardinalities, embedding_dims)])\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.num_features > 1:\n",
    "            # we slice the last dimension, giving an array of length\n",
    "            # self.num_features with shape (N,T) or (N)\n",
    "            cat_feature_slices = torch.chunk(features, self.num_features, dim=-1)\n",
    "        else:\n",
    "            cat_feature_slices = [features]\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                embed(cat_feature_slice.squeeze(-1))\n",
    "                for embed, cat_feature_slice in zip(self.embedders, cat_feature_slices)\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "\n",
    "class TimeSeriesStdScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Standardize features by calculating the mean and scaling along some given dimension `dim`, and then normalizes it\n",
    "    by subtracting from the mean and dividing by the standard deviation.\n",
    "\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to calculate the mean and standard deviation.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-5):\n",
    "            Default scale that is used for elements that are constantly zero along dimension `dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False, minimum_scale: float = 1e-5):\n",
    "        super().__init__()\n",
    "        if not dim > 0:\n",
    "            raise ValueError(\"Cannot compute scale along dim = 0 (batch dimension), please provide dim > 0\")\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        denominator = weights.sum(self.dim, keepdim=self.keepdim)\n",
    "        denominator = denominator.clamp_min(1.0)\n",
    "        loc = (data * weights).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "\n",
    "        variance = (((data - loc) * weights) ** 2).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "        scale = torch.sqrt(variance + self.minimum_scale)\n",
    "        return (data - loc) / scale, loc, scale\n",
    "\n",
    "\n",
    "class TimeSeriesMeanScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a scaling factor as the weighted average absolute value along dimension `dim`, and scales the data\n",
    "    accordingly.\n",
    "\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        default_scale (`float`, *optional*, defaults to `None`):\n",
    "            Default scale that is used for elements that are constantly zero. If `None`, we use the scale of the batch.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-10):\n",
    "            Default minimum possible scale that is used for any item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim: int = -1, keepdim: bool = True, default_scale: Optional[float] = None, minimum_scale: float = 1e-10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "        self.default_scale = default_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self, data: torch.Tensor, observed_indicator: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # shape: (N, [C], T=1)\n",
    "        ts_sum = (data * observed_indicator).abs().sum(self.dim, keepdim=True)\n",
    "        num_observed = observed_indicator.sum(self.dim, keepdim=True)\n",
    "\n",
    "        scale = ts_sum / torch.clamp(num_observed, min=1)\n",
    "\n",
    "        # If `default_scale` is provided, we use it, otherwise we use the scale\n",
    "        # of the batch.\n",
    "        if self.default_scale is None:\n",
    "            batch_sum = ts_sum.sum(dim=0)\n",
    "            batch_observations = torch.clamp(num_observed.sum(0), min=1)\n",
    "            default_scale = torch.squeeze(batch_sum / batch_observations)\n",
    "        else:\n",
    "            default_scale = self.default_scale * torch.ones_like(scale)\n",
    "\n",
    "        # apply default scale where there are no observations\n",
    "        scale = torch.where(num_observed > 0, scale, default_scale)\n",
    "\n",
    "        # ensure the scale is at least `self.minimum_scale`\n",
    "        scale = torch.clamp(scale, min=self.minimum_scale)\n",
    "        scaled_data = data / scale\n",
    "\n",
    "        if not self.keepdim:\n",
    "            scale = scale.squeeze(dim=self.dim)\n",
    "\n",
    "        return scaled_data, torch.zeros_like(scale), scale\n",
    "\n",
    "\n",
    "class TimeSeriesNOPScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Assigns a scaling factor equal to 1 along dimension `dim`, and therefore applies no scaling to the input data.\n",
    "\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "\n",
    "    def forward(\n",
    "        self, data: torch.Tensor, observed_indicator: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        scale = torch.ones_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        loc = torch.zeros_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        return data, loc, scale\n",
    "\n",
    "\n",
    "def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the negative log likelihood loss from input distribution with respect to target.\n",
    "    \"\"\"\n",
    "    return -input.log_prob(target)\n",
    "\n",
    "\n",
    "def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (`torch.FloatTensor`):\n",
    "            Input tensor, of which the average must be computed.\n",
    "        weights (`torch.FloatTensor`, *optional*):\n",
    "            Weights tensor, of the same shape as `input_tensor`.\n",
    "        dim (`int`, *optional*):\n",
    "            The dim along which to average `input_tensor`.\n",
    "\n",
    "    Returns:\n",
    "        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n",
    "        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n",
    "        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n",
    "    else:\n",
    "        return input_tensor.mean(dim=dim)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->TimeSeries\n",
    "class TimeSeriesSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter) -> nn.Parameter:\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions)\n",
    "\n",
    "\n",
    "class TimeSeriesValueEmbedding(nn.Module):\n",
    "    def __init__(self, feature_size, d_model):\n",
    "        super().__init__()\n",
    "        self.value_projection = nn.Linear(in_features=feature_size, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.value_projection(x)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->TimeSeriesTransformer\n",
    "class TimeSeriesTransformerAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->TimeSeriesTransformer\n",
    "class TimeSeriesTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: TimeSeriesTransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        self.self_attn = TimeSeriesTransformerAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.encoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "        )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: torch.FloatTensor,\n",
    "        layer_head_mask: torch.FloatTensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        hidden_states, attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if hidden_states.dtype == torch.float16 and (\n",
    "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
    "        ):\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->TimeSeriesTransformer\n",
    "class TimeSeriesTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TimeSeriesTransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "\n",
    "        self.self_attn = TimeSeriesTransformerAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.encoder_attn = TimeSeriesTransformerAttention(\n",
    "            self.embed_dim,\n",
    "            config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
    "                size `(decoder_attention_heads,)`.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Self Attention\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_present_key_value = None\n",
    "        cross_attn_weights = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                layer_head_mask=cross_attn_layer_head_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TimeSeriesTransformerPreTrainedModel(PreTrainedModel):\n",
    "    config_class = TimeSeriesTransformerConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    main_input_name = \"past_values\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, TimeSeriesSinusoidalPositionalEmbedding):\n",
    "            pass\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "\n",
    "TIME_SERIES_TRANSFORMER_START_DOCSTRING = r\"\"\"\n",
    "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
    "    etc.)\n",
    "\n",
    "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
    "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
    "    and behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`TimeSeriesTransformerConfig`]):\n",
    "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
    "            load the weights associated with the model, only the configuration. Check out the\n",
    "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "TIME_SERIES_TRANSFORMER_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n",
    "            Past values of the time series, that serve as context in order to predict the future. The sequence size of\n",
    "            this tensor must be larger than the `context_length` of the model, since the model will use the larger size\n",
    "            to construct lag features, i.e. additional values from the past which are added in order to serve as \"extra\n",
    "            context\".\n",
    "\n",
    "            The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`, which if no\n",
    "            `lags_sequence` is configured, is equal to `config.context_length` + 7 (as by default, the largest\n",
    "            look-back index in `config.lags_sequence` is 7). The property `_past_length` returns the actual length of\n",
    "            the past.\n",
    "\n",
    "            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n",
    "            `static_categorical_features`, `static_real_features`, `past_time_features` and lags).\n",
    "\n",
    "            Optionally, missing values need to be replaced with zeros and indicated via the `past_observed_mask`.\n",
    "\n",
    "            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n",
    "            variates in the time series per time step.\n",
    "        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`):\n",
    "            Required time features, which the model internally will add to `past_values`. These could be things like\n",
    "            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n",
    "            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n",
    "            time-series is. Age features have small values for distant past time steps and increase monotonically the\n",
    "            more we approach the current time step. Holiday features are also a good example of time features.\n",
    "\n",
    "            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n",
    "            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n",
    "            Transformer requires to provide additional time features. The Time Series Transformer only learns\n",
    "            additional embeddings for `static_categorical_features`.\n",
    "\n",
    "            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n",
    "            must but known at prediction time.\n",
    "\n",
    "            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
    "        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
    "            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n",
    "            `[0, 1]`:\n",
    "\n",
    "            - 1 for values that are **observed**,\n",
    "            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
    "\n",
    "        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n",
    "            Optional static categorical features for which the model will learn an embedding, which it will add to the\n",
    "            values of the time series.\n",
    "\n",
    "            Static categorical features are features which have the same value for all time steps (static over time).\n",
    "\n",
    "            A typical example of a static categorical feature is a time series ID.\n",
    "        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n",
    "            Optional static real features which the model will add to the values of the time series.\n",
    "\n",
    "            Static real features are features which have the same value for all time steps (static over time).\n",
    "\n",
    "            A typical example of a static real feature is promotion information.\n",
    "        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)` or `(batch_size, prediction_length, input_size)`, *optional*):\n",
    "            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n",
    "            Transformer needs during training to learn to output, given the `past_values`.\n",
    "\n",
    "            The sequence length here is equal to `prediction_length`.\n",
    "\n",
    "            See the demo notebook and code snippets for details.\n",
    "\n",
    "            Optionally, during training any missing values need to be replaced with zeros and indicated via the\n",
    "            `future_observed_mask`.\n",
    "\n",
    "            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n",
    "            variates in the time series per time step.\n",
    "        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`):\n",
    "            Required time features for the prediction window, which the model internally will add to `future_values`.\n",
    "            These could be things like \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as\n",
    "            Fourier features). These could also be so-called \"age\" features, which basically help the model know \"at\n",
    "            which point in life\" a time-series is. Age features have small values for distant past time steps and\n",
    "            increase monotonically the more we approach the current time step. Holiday features are also a good example\n",
    "            of time features.\n",
    "\n",
    "            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n",
    "            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n",
    "            Transformer requires to provide additional time features. The Time Series Transformer only learns\n",
    "            additional embeddings for `static_categorical_features`.\n",
    "\n",
    "            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n",
    "            must but known at prediction time.\n",
    "\n",
    "            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
    "        future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
    "            Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n",
    "            in `[0, 1]`:\n",
    "\n",
    "            - 1 for values that are **observed**,\n",
    "            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
    "\n",
    "            This mask is used to filter out missing values for the final loss calculation.\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on certain token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, to\n",
    "            make sure the model can only look at previous inputs in order to predict the future.\n",
    "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
    "            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n",
    "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n",
    "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
    "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
    "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
    "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
    "            model's internal embedding lookup matrix.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TimeSeriesTransformerEncoder(TimeSeriesTransformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`TimeSeriesTransformerEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: TimeSeriesTransformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TimeSeriesTransformerConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "        if config.prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = TimeSeriesValueEmbedding(feature_size=config.feature_size, d_model=config.d_model)\n",
    "        self.embed_positions = TimeSeriesSinusoidalPositionalEmbedding(\n",
    "            config.context_length + config.prediction_length, config.d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([TimeSeriesTransformerEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size())\n",
    "\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            to_drop = False\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:  # skip the layer\n",
    "                    to_drop = True\n",
    "\n",
    "            if to_drop:\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "                    layer_outputs = self._gradient_checkpointing_func(\n",
    "                        encoder_layer.__call__,\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "\n",
    "class TimeSeriesTransformerDecoder(TimeSeriesTransformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a\n",
    "    [`TimeSeriesTransformerDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: TimeSeriesTransformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TimeSeriesTransformerConfig):\n",
    "        super().__init__(config)\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.decoder_layerdrop\n",
    "        if config.prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = TimeSeriesValueEmbedding(feature_size=config.feature_size, d_model=config.d_model)\n",
    "        self.embed_positions = TimeSeriesSinusoidalPositionalEmbedding(\n",
    "            config.context_length + config.prediction_length, config.d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([TimeSeriesTransformerDecoderLayer(config) for _ in range(config.decoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                of the decoder.\n",
    "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
    "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
    "                selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n",
    "                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            encoder_attention_mask = _prepare_4d_attention_mask(\n",
    "                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
    "            )\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size(), past_key_values_length=self.config.context_length)\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                if attn_mask.size()[0] != (len(self.layers)):\n",
    "                    raise ValueError(\n",
    "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                        f\" {head_mask.size()[0]}.\"\n",
    "                    )\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:\n",
    "                    continue\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    head_mask[idx] if head_mask is not None else None,\n",
    "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
    "                    None,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    cross_attn_layer_head_mask=(\n",
    "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
    "                    ),\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare Time Series Transformer Model outputting raw hidden-states without any specific head on top.\",\n",
    "    TIME_SERIES_TRANSFORMER_START_DOCSTRING,\n",
    ")\n",
    "class TimeSeriesTransformerModel(TimeSeriesTransformerPreTrainedModel):\n",
    "    def __init__(self, config: TimeSeriesTransformerConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.scaling == \"mean\" or config.scaling is True:\n",
    "            self.scaler = TimeSeriesMeanScaler(dim=1, keepdim=True)\n",
    "        elif config.scaling == \"std\":\n",
    "            self.scaler = TimeSeriesStdScaler(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.scaler = TimeSeriesNOPScaler(dim=1, keepdim=True)\n",
    "\n",
    "        if config.num_static_categorical_features > 0:\n",
    "            self.embedder = TimeSeriesFeatureEmbedder(\n",
    "                cardinalities=config.cardinality,\n",
    "                embedding_dims=config.embedding_dimension,\n",
    "            )\n",
    "\n",
    "        # transformer encoder-decoder and mask initializer\n",
    "        self.encoder = TimeSeriesTransformerEncoder(config)\n",
    "        self.decoder = TimeSeriesTransformerDecoder(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @property\n",
    "    def _past_length(self) -> int:\n",
    "        return self.config.context_length + max(self.config.lags_sequence)\n",
    "\n",
    "    def get_lagged_subsequences(\n",
    "        self, sequence: torch.Tensor, subsequences_length: int, shift: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns lagged subsequences of a given sequence. Returns a tensor of shape (N, S, C, I),\n",
    "            where S = subsequences_length and I = len(indices), containing lagged subsequences. Specifically, lagged[i,\n",
    "            j, :, k] = sequence[i, -indices[k]-S+j, :].\n",
    "\n",
    "        Args:\n",
    "            sequence: Tensor\n",
    "                The sequence from which lagged subsequences should be extracted. Shape: (N, T, C).\n",
    "            subsequences_length : int\n",
    "                Length of the subsequences to be extracted.\n",
    "            shift: int\n",
    "                Shift the lags by this amount back.\n",
    "        \"\"\"\n",
    "        sequence_length = sequence.shape[1]\n",
    "        indices = [lag - shift for lag in self.config.lags_sequence]\n",
    "\n",
    "        if max(indices) + subsequences_length > sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"lags cannot go further than history length, found lag {max(indices)} \"\n",
    "                f\"while history length is only {sequence_length}\"\n",
    "            )\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...])\n",
    "        return torch.stack(lagged_values, dim=-1)\n",
    "\n",
    "    def create_network_inputs(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        past_observed_mask: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        # time feature\n",
    "        time_feat = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    past_time_features[:, self._past_length - self.config.context_length :, ...],\n",
    "                    future_time_features,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            if future_values is not None\n",
    "            else past_time_features[:, self._past_length - self.config.context_length :, ...]\n",
    "        )\n",
    "\n",
    "        # target\n",
    "        if past_observed_mask is None:\n",
    "            past_observed_mask = torch.ones_like(past_values)\n",
    "\n",
    "        context = past_values[:, -self.config.context_length :]\n",
    "        observed_context = past_observed_mask[:, -self.config.context_length :]\n",
    "        _, loc, scale = self.scaler(context, observed_context)\n",
    "\n",
    "        inputs = (\n",
    "            (torch.cat((past_values, future_values), dim=1) - loc) / scale\n",
    "            if future_values is not None\n",
    "            else (past_values - loc) / scale\n",
    "        )\n",
    "\n",
    "        # static features\n",
    "        log_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p()\n",
    "        log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n",
    "        static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n",
    "\n",
    "        if static_real_features is not None:\n",
    "            static_feat = torch.cat((static_real_features, static_feat), dim=1)\n",
    "        if static_categorical_features is not None:\n",
    "            embedded_cat = self.embedder(static_categorical_features)\n",
    "            static_feat = torch.cat((embedded_cat, static_feat), dim=1)\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\n",
    "\n",
    "        # all features\n",
    "        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n",
    "\n",
    "        # lagged features\n",
    "        subsequences_length = (\n",
    "            self.config.context_length + self.config.prediction_length\n",
    "            if future_values is not None\n",
    "            else self.config.context_length\n",
    "        )\n",
    "        lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\n",
    "        lags_shape = lagged_sequence.shape\n",
    "        reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n",
    "\n",
    "        if reshaped_lagged_sequence.shape[1] != time_feat.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"input length {reshaped_lagged_sequence.shape[1]} and time feature lengths {time_feat.shape[1]} does not match\"\n",
    "            )\n",
    "\n",
    "        # transformer inputs\n",
    "        transformer_inputs = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n",
    "\n",
    "        return transformer_inputs, loc, scale, static_feat\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(TIME_SERIES_TRANSFORMER_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqTSModelOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask: torch.Tensor,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Seq2SeqTSModelOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from huggingface_hub import hf_hub_download\n",
    "        >>> import torch\n",
    "        >>> from transformers import TimeSeriesTransformerModel\n",
    "\n",
    "        >>> file = hf_hub_download(\n",
    "        ...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    "        ... )\n",
    "        >>> batch = torch.load(file)\n",
    "\n",
    "        >>> model = TimeSeriesTransformerModel.from_pretrained(\"huggingface/time-series-transformer-tourism-monthly\")\n",
    "\n",
    "        >>> # during training, one provides both past and future values\n",
    "        >>> # as well as possible additional features\n",
    "        >>> outputs = model(\n",
    "        ...     past_values=batch[\"past_values\"],\n",
    "        ...     past_time_features=batch[\"past_time_features\"],\n",
    "        ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "        ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "        ...     static_real_features=batch[\"static_real_features\"],\n",
    "        ...     future_values=batch[\"future_values\"],\n",
    "        ...     future_time_features=batch[\"future_time_features\"],\n",
    "        ... )\n",
    "\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            static_categorical_features=static_categorical_features,\n",
    "            static_real_features=static_real_features,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "        )\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            enc_input = transformer_inputs[:, : self.config.context_length, ...]\n",
    "            encoder_outputs = self.encoder(\n",
    "                inputs_embeds=enc_input,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        dec_input = transformer_inputs[:, self.config.context_length :, ...]\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=dec_input,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs + encoder_outputs + (loc, scale, static_feat)\n",
    "\n",
    "        return Seq2SeqTSModelOutput(\n",
    "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "            loc=loc,\n",
    "            scale=scale,\n",
    "            static_features=static_feat,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The Time Series Transformer Model with a distribution head on top for time-series forecasting.\",\n",
    "    TIME_SERIES_TRANSFORMER_START_DOCSTRING,\n",
    ")\n",
    "class TimeSeriesTransformerForPrediction(TimeSeriesTransformerPreTrainedModel):\n",
    "    def __init__(self, config: TimeSeriesTransformerConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = TimeSeriesTransformerModel(config)\n",
    "        if config.distribution_output == \"student_t\":\n",
    "            self.distribution_output = StudentTOutput(dim=config.input_size)\n",
    "        elif config.distribution_output == \"normal\":\n",
    "            self.distribution_output = NormalOutput(dim=config.input_size)\n",
    "        elif config.distribution_output == \"negative_binomial\":\n",
    "            self.distribution_output = NegativeBinomialOutput(dim=config.input_size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distribution output {config.distribution_output}\")\n",
    "\n",
    "        self.parameter_projection = self.distribution_output.get_parameter_projection(self.model.config.d_model)\n",
    "        self.target_shape = self.distribution_output.event_shape\n",
    "\n",
    "        if config.loss == \"nll\":\n",
    "            self.loss = nll\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function {config.loss}\")\n",
    "\n",
    "        # Initialize weights of distribution_output and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def output_params(self, dec_output):\n",
    "        return self.parameter_projection(dec_output)\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.model.get_encoder()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.get_decoder()\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def output_distribution(self, params, loc=None, scale=None, trailing_n=None) -> torch.distributions.Distribution:\n",
    "        sliced_params = params\n",
    "        if trailing_n is not None:\n",
    "            sliced_params = [p[:, -trailing_n:] for p in params]\n",
    "        return self.distribution_output.distribution(sliced_params, loc=loc, scale=scale)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(TIME_SERIES_TRANSFORMER_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqTSModelOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        past_observed_mask: torch.Tensor,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "        future_observed_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Seq2SeqTSModelOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from huggingface_hub import hf_hub_download\n",
    "        >>> import torch\n",
    "        >>> from transformers import TimeSeriesTransformerForPrediction\n",
    "\n",
    "        >>> file = hf_hub_download(\n",
    "        ...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    "        ... )\n",
    "        >>> batch = torch.load(file)\n",
    "\n",
    "        >>> model = TimeSeriesTransformerForPrediction.from_pretrained(\n",
    "        ...     \"huggingface/time-series-transformer-tourism-monthly\"\n",
    "        ... )\n",
    "\n",
    "        >>> # during training, one provides both past and future values\n",
    "        >>> # as well as possible additional features\n",
    "        >>> outputs = model(\n",
    "        ...     past_values=batch[\"past_values\"],\n",
    "        ...     past_time_features=batch[\"past_time_features\"],\n",
    "        ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "        ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "        ...     static_real_features=batch[\"static_real_features\"],\n",
    "        ...     future_values=batch[\"future_values\"],\n",
    "        ...     future_time_features=batch[\"future_time_features\"],\n",
    "        ... )\n",
    "\n",
    "        >>> loss = outputs.loss\n",
    "        >>> loss.backward()\n",
    "\n",
    "        >>> # during inference, one only provides past values\n",
    "        >>> # as well as possible additional features\n",
    "        >>> # the model autoregressively generates future values\n",
    "        >>> outputs = model.generate(\n",
    "        ...     past_values=batch[\"past_values\"],\n",
    "        ...     past_time_features=batch[\"past_time_features\"],\n",
    "        ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "        ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "        ...     static_real_features=batch[\"static_real_features\"],\n",
    "        ...     future_time_features=batch[\"future_time_features\"],\n",
    "        ... )\n",
    "\n",
    "        >>> mean_prediction = outputs.sequences.mean(dim=1)\n",
    "        ```\"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if future_values is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.model(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            static_categorical_features=static_categorical_features,\n",
    "            static_real_features=static_real_features,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        prediction_loss = None\n",
    "        params = None\n",
    "        if future_values is not None:\n",
    "            params = self.output_params(outputs[0])  # outputs.last_hidden_state\n",
    "            # loc is 3rd last and scale is 2nd last output\n",
    "            distribution = self.output_distribution(params, loc=outputs[-3], scale=outputs[-2])\n",
    "\n",
    "            loss = self.loss(distribution, future_values)\n",
    "\n",
    "            if future_observed_mask is None:\n",
    "                future_observed_mask = torch.ones_like(future_values)\n",
    "\n",
    "            if len(self.target_shape) == 0:\n",
    "                loss_weights = future_observed_mask\n",
    "            else:\n",
    "                loss_weights, _ = future_observed_mask.min(dim=-1, keepdim=False)\n",
    "\n",
    "            prediction_loss = weighted_average(loss, weights=loss_weights)\n",
    "\n",
    "        if not return_dict:\n",
    "            outputs = ((params,) + outputs[1:]) if params is not None else outputs[1:]\n",
    "            return ((prediction_loss,) + outputs) if prediction_loss is not None else outputs\n",
    "\n",
    "        return Seq2SeqTSPredictionOutput(\n",
    "            loss=prediction_loss,\n",
    "            params=params,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "            loc=outputs.loc,\n",
    "            scale=outputs.scale,\n",
    "            static_features=outputs.static_features,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        future_time_features: torch.Tensor,\n",
    "        past_observed_mask: Optional[torch.Tensor] = None,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> SampleTSPredictionOutput:\n",
    "        r\"\"\"\n",
    "        Greedily generate sequences of sample predictions from a model with a probability distribution head.\n",
    "\n",
    "        Parameters:\n",
    "            past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n",
    "                Past values of the time series, that serve as context in order to predict the future. The sequence size\n",
    "                of this tensor must be larger than the `context_length` of the model, since the model will use the\n",
    "                larger size to construct lag features, i.e. additional values from the past which are added in order to\n",
    "                serve as \"extra context\".\n",
    "\n",
    "                The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`, which if\n",
    "                no `lags_sequence` is configured, is equal to `config.context_length` + 7 (as by default, the largest\n",
    "                look-back index in `config.lags_sequence` is 7). The property `_past_length` returns the actual length\n",
    "                of the past.\n",
    "\n",
    "                The `past_values` is what the Transformer encoder gets as input (with optional additional features,\n",
    "                such as `static_categorical_features`, `static_real_features`, `past_time_features` and lags).\n",
    "\n",
    "                Optionally, missing values need to be replaced with zeros and indicated via the `past_observed_mask`.\n",
    "\n",
    "                For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number\n",
    "                of variates in the time series per time step.\n",
    "            past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`):\n",
    "                Required time features, which the model internally will add to `past_values`. These could be things\n",
    "                like \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features).\n",
    "                These could also be so-called \"age\" features, which basically help the model know \"at which point in\n",
    "                life\" a time-series is. Age features have small values for distant past time steps and increase\n",
    "                monotonically the more we approach the current time step. Holiday features are also a good example of\n",
    "                time features.\n",
    "\n",
    "                These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT,\n",
    "                where the position encodings are learned from scratch internally as parameters of the model, the Time\n",
    "                Series Transformer requires to provide additional time features. The Time Series Transformer only\n",
    "                learns additional embeddings for `static_categorical_features`.\n",
    "\n",
    "                Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these\n",
    "                features must but known at prediction time.\n",
    "\n",
    "                The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
    "            future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`):\n",
    "                Required time features for the prediction window, which the model internally will add to sampled\n",
    "                predictions. These could be things like \"month of year\", \"day of the month\", etc. encoded as vectors\n",
    "                (for instance as Fourier features). These could also be so-called \"age\" features, which basically help\n",
    "                the model know \"at which point in life\" a time-series is. Age features have small values for distant\n",
    "                past time steps and increase monotonically the more we approach the current time step. Holiday features\n",
    "                are also a good example of time features.\n",
    "\n",
    "                These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT,\n",
    "                where the position encodings are learned from scratch internally as parameters of the model, the Time\n",
    "                Series Transformer requires to provide additional time features. The Time Series Transformer only\n",
    "                learns additional embeddings for `static_categorical_features`.\n",
    "\n",
    "                Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these\n",
    "                features must but known at prediction time.\n",
    "\n",
    "                The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
    "            past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
    "                Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected\n",
    "                in `[0, 1]`:\n",
    "\n",
    "                - 1 for values that are **observed**,\n",
    "                - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
    "\n",
    "            static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n",
    "                Optional static categorical features for which the model will learn an embedding, which it will add to\n",
    "                the values of the time series.\n",
    "\n",
    "                Static categorical features are features which have the same value for all time steps (static over\n",
    "                time).\n",
    "\n",
    "                A typical example of a static categorical feature is a time series ID.\n",
    "            static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n",
    "                Optional static real features which the model will add to the values of the time series.\n",
    "\n",
    "                Static real features are features which have the same value for all time steps (static over time).\n",
    "\n",
    "                A typical example of a static real feature is promotion information.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers.\n",
    "\n",
    "        Return:\n",
    "            [`SampleTSPredictionOutput`] where the outputs `sequences` tensor will have shape `(batch_size, number of\n",
    "            samples, prediction_length)` or `(batch_size, number of samples, prediction_length, input_size)` for\n",
    "            multivariate predictions.\n",
    "        \"\"\"\n",
    "        outputs = self(\n",
    "            static_categorical_features=static_categorical_features,\n",
    "            static_real_features=static_real_features,\n",
    "            past_time_features=past_time_features,\n",
    "            past_values=past_values,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_time_features=future_time_features,\n",
    "            future_values=None,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "        decoder = self.model.get_decoder()\n",
    "        enc_last_hidden = outputs.encoder_last_hidden_state\n",
    "        loc = outputs.loc\n",
    "        scale = outputs.scale\n",
    "        static_feat = outputs.static_features\n",
    "\n",
    "        num_parallel_samples = self.config.num_parallel_samples\n",
    "        repeated_loc = loc.repeat_interleave(repeats=num_parallel_samples, dim=0)\n",
    "        repeated_scale = scale.repeat_interleave(repeats=num_parallel_samples, dim=0)\n",
    "\n",
    "        repeated_past_values = (\n",
    "            past_values.repeat_interleave(repeats=num_parallel_samples, dim=0) - repeated_loc\n",
    "        ) / repeated_scale\n",
    "\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, future_time_features.shape[1], -1)\n",
    "        features = torch.cat((expanded_static_feat, future_time_features), dim=-1)\n",
    "        repeated_features = features.repeat_interleave(repeats=num_parallel_samples, dim=0)\n",
    "\n",
    "        repeated_enc_last_hidden = enc_last_hidden.repeat_interleave(repeats=num_parallel_samples, dim=0)\n",
    "\n",
    "        future_samples = []\n",
    "\n",
    "        # greedy decoding\n",
    "        for k in range(self.config.prediction_length):\n",
    "            lagged_sequence = self.model.get_lagged_subsequences(\n",
    "                sequence=repeated_past_values,\n",
    "                subsequences_length=1 + k,\n",
    "                shift=1,\n",
    "            )\n",
    "\n",
    "            lags_shape = lagged_sequence.shape\n",
    "            reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n",
    "\n",
    "            decoder_input = torch.cat((reshaped_lagged_sequence, repeated_features[:, : k + 1]), dim=-1)\n",
    "\n",
    "            dec_output = decoder(inputs_embeds=decoder_input, encoder_hidden_states=repeated_enc_last_hidden)\n",
    "            dec_last_hidden = dec_output.last_hidden_state\n",
    "\n",
    "            params = self.parameter_projection(dec_last_hidden[:, -1:])\n",
    "            distr = self.output_distribution(params, loc=repeated_loc, scale=repeated_scale)\n",
    "            next_sample = distr.sample()\n",
    "\n",
    "            repeated_past_values = torch.cat(\n",
    "                (repeated_past_values, (next_sample - repeated_loc) / repeated_scale), dim=1\n",
    "            )\n",
    "            future_samples.append(next_sample)\n",
    "\n",
    "        concat_future_samples = torch.cat(future_samples, dim=1)\n",
    "\n",
    "        return SampleTSPredictionOutput(\n",
    "            sequences=concat_future_samples.reshape(\n",
    "                (-1, num_parallel_samples, self.config.prediction_length) + self.target_shape,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d659536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c6f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df561259",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a42e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3ca8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gluonts ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa697e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc25bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"time_series_transformers_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4c556f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39e7890dbb04017ae5c68dc219cb401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36c4e7f76114556b95ec85440bd99ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51b33221c00435684f7fe83b5e6f82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/7.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8184c599b15143a89c792418a270a195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/200k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2655a83d84c5420bac914793cd07831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b34dc7157e49f580d16ad3524e653c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127d8e3fb06b40b0aeff840ab13e4895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"monash_tsf\", \"tourism_monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d0156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "        num_rows: 366\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "        num_rows: 366\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "        num_rows: 366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfdb4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_example = dataset[\"train\"][0]\n",
    "train_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0223c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-01-01 00:00:00\n",
      "[1149.8699951171875, 1053.8001708984375, 1388.8797607421875, 1783.3702392578125, 1921.025146484375, 2704.94482421875, 4184.41357421875, 4148.35400390625, 2620.72509765625, 1650.300048828125, 1115.9200439453125, 1370.6251220703125, 1096.31494140625, 978.4600219726562, 1294.68505859375, 1480.465087890625, 1748.865234375, 2216.920166015625, 4690.5185546875, 4682.8642578125, 2459.579833984375, 1484.4901123046875, 1028.985107421875, 1109.3648681640625, 960.8751220703125, 896.35009765625, 1118.6551513671875, 1619.9949951171875, 1847.994873046875, 2367.044921875, 4991.16015625, 4772.9443359375, 2894.678466796875, 1860.4801025390625, 1185.150146484375, 1313.659912109375, 1160.9150390625, 1061.5048828125, 1301.77001953125, 1794.3797607421875, 2106.455078125, 2789.034912109375, 4917.8466796875, 4994.4833984375, 3016.754150390625, 1941.505126953125, 1234.135009765625, 1378.72021484375, 1182.9749755859375, 1081.6600341796875, 1424.110107421875, 1774.5350341796875, 2115.420166015625, 2804.840087890625, 4849.498046875, 4937.47509765625, 3074.2236328125, 2063.42529296875, 1297.355224609375, 1350.710205078125, 1224.360107421875, 1165.815185546875, 1409.3299560546875, 2116.5498046875, 2357.135009765625, 2995.0703125, 5295.2119140625, 4957.90478515625, 3321.959228515625, 2221.18017578125, 1345.9000244140625, 1514.01513671875, 1239.5501708984375, 1172.159912109375, 1518.9752197265625, 1996.8751220703125, 2248.68505859375, 3053.440185546875, 5019.45361328125, 5466.7802734375, 3235.167724609375, 2157.97998046875, 1379.7252197265625, 1728.0400390625, 1350.10986328125, 1216.014892578125, 1751.3251953125, 1805.320068359375, 2570.02490234375, 3204.240234375, 5395.72021484375, 6078.82861328125, 3587.098388671875, 2285.195068359375, 1582.18994140625, 1787.4298095703125, 1554.8701171875, 1409.8648681640625, 1612.125, 2286.239990234375, 2913.755126953125, 3645.908447265625, 5956.70849609375, 6326.97509765625, 3914.66015625, 2617.675048828125, 1675.1650390625, 2139.219970703125, 1715.4898681640625, 1663.5799560546875, 2053.699951171875, 2354.929931640625, 3038.591796875, 3470.609375, 6606.18359375, 6587.63671875, 4133.78271484375, 2960.0244140625, 1762.5849609375, 2125.64013671875, 1815.9150390625, 1632.31494140625, 2210.39501953125, 2210.215087890625, 3099.269287109375, 3468.77783203125, 6482.92529296875, 6665.48486328125, 4006.36181640625, 2882.3349609375, 1775.2498779296875, 2171.64990234375, 1796.4749755859375, 1692.349853515625, 1949.78515625, 2680.630126953125, 2645.949951171875, 3414.742919921875, 5772.876953125]\n"
     ]
    }
   ],
   "source": [
    "print(train_example[\"start\"])\n",
    "print(train_example[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f989fa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_example = dataset[\"validation\"][0]\n",
    "validation_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ea6907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-01-01 00:00:00\n",
      "[1149.8699951171875, 1053.8001708984375, 1388.8797607421875, 1783.3702392578125, 1921.025146484375, 2704.94482421875, 4184.41357421875, 4148.35400390625, 2620.72509765625, 1650.300048828125, 1115.9200439453125, 1370.6251220703125, 1096.31494140625, 978.4600219726562, 1294.68505859375, 1480.465087890625, 1748.865234375, 2216.920166015625, 4690.5185546875, 4682.8642578125, 2459.579833984375, 1484.4901123046875, 1028.985107421875, 1109.3648681640625, 960.8751220703125, 896.35009765625, 1118.6551513671875, 1619.9949951171875, 1847.994873046875, 2367.044921875, 4991.16015625, 4772.9443359375, 2894.678466796875, 1860.4801025390625, 1185.150146484375, 1313.659912109375, 1160.9150390625, 1061.5048828125, 1301.77001953125, 1794.3797607421875, 2106.455078125, 2789.034912109375, 4917.8466796875, 4994.4833984375, 3016.754150390625, 1941.505126953125, 1234.135009765625, 1378.72021484375, 1182.9749755859375, 1081.6600341796875, 1424.110107421875, 1774.5350341796875, 2115.420166015625, 2804.840087890625, 4849.498046875, 4937.47509765625, 3074.2236328125, 2063.42529296875, 1297.355224609375, 1350.710205078125, 1224.360107421875, 1165.815185546875, 1409.3299560546875, 2116.5498046875, 2357.135009765625, 2995.0703125, 5295.2119140625, 4957.90478515625, 3321.959228515625, 2221.18017578125, 1345.9000244140625, 1514.01513671875, 1239.5501708984375, 1172.159912109375, 1518.9752197265625, 1996.8751220703125, 2248.68505859375, 3053.440185546875, 5019.45361328125, 5466.7802734375, 3235.167724609375, 2157.97998046875, 1379.7252197265625, 1728.0400390625, 1350.10986328125, 1216.014892578125, 1751.3251953125, 1805.320068359375, 2570.02490234375, 3204.240234375, 5395.72021484375, 6078.82861328125, 3587.098388671875, 2285.195068359375, 1582.18994140625, 1787.4298095703125, 1554.8701171875, 1409.8648681640625, 1612.125, 2286.239990234375, 2913.755126953125, 3645.908447265625, 5956.70849609375, 6326.97509765625, 3914.66015625, 2617.675048828125, 1675.1650390625, 2139.219970703125, 1715.4898681640625, 1663.5799560546875, 2053.699951171875, 2354.929931640625, 3038.591796875, 3470.609375, 6606.18359375, 6587.63671875, 4133.78271484375, 2960.0244140625, 1762.5849609375, 2125.64013671875, 1815.9150390625, 1632.31494140625, 2210.39501953125, 2210.215087890625, 3099.269287109375, 3468.77783203125, 6482.92529296875, 6665.48486328125, 4006.36181640625, 2882.3349609375, 1775.2498779296875, 2171.64990234375, 1796.4749755859375, 1692.349853515625, 1949.78515625, 2680.630126953125, 2645.949951171875, 3414.742919921875, 5772.876953125, 6053.7041015625, 3878.12841796875, 2806.514892578125, 1735.5382080078125, 2128.919921875, 1608.01416015625, 1441.330078125, 2068.235107421875, 2207.610107421875, 2918.409912109375, 3400.81787109375, 6048.7421875, 6483.14013671875, 4063.502685546875, 2900.22998046875, 1907.094970703125, 2338.510009765625, 1787.1650390625, 1699.6451416015625, 1979.105224609375, 2824.260009765625, 3076.5048828125, 3402.5849609375, 5985.830078125]\n"
     ]
    }
   ],
   "source": [
    "print(validation_example[\"start\"])\n",
    "print(validation_example[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8881e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1M\"\n",
    "prediction_length = 24\n",
    "\n",
    "assert len(train_example[\"target\"]) + prediction_length == len(\n",
    "    validation_example[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71c8e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "axes.plot(train_example[\"target\"], color=\"blue\")\n",
    "axes.plot(validation_example[\"target\"], color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "010f5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7251c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@lru_cache(10_000)\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c61af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field, freq=freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f5792c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 23, 24, 25, 35, 36, 37]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "print(lags_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags coming from helper given the freq:\n",
    "    lags_sequence=lags_sequence,\n",
    "    # we'll add 2 time features (\"month of year\" and \"age\", see further):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # we have a single static categorical feature, namely time series ID:\n",
    "    num_static_categorical_features=1,\n",
    "    # it has 366 possible values:\n",
    "    cardinality=[len(train_dataset)],\n",
    "    # the model will learn an embedding of size 2 for each of the 366 possible values:\n",
    "    embedding_dimension=[2],\n",
    "    \n",
    "    # transformer params:\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32,\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9117820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

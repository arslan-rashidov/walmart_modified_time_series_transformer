{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2e3aa0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-macosx_12_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m524.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m657.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m330.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1642,
   "id": "cd7469dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-1.0.0-py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi>=0.80 (from pytorch-forecasting)\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
      "  Downloading lightning-2.1.1-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pytorch-forecasting) (3.7.3)\n",
      "Collecting optuna<4.0.0,>=3.1.0 (from pytorch-forecasting)\n",
      "  Downloading optuna-3.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pandas<=3.0.0,>=1.3.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pytorch-forecasting) (2.0.3)\n",
      "Collecting pytorch-optimizer<3.0.0,>=2.5.1 (from pytorch-forecasting)\n",
      "  Downloading pytorch_optimizer-2.12.0-py3-none-any.whl.metadata (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<2.0,>=1.2 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pytorch-forecasting) (1.3.2)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pytorch-forecasting) (1.10.1)\n",
      "Collecting statsmodels (from pytorch-forecasting)\n",
      "  Downloading statsmodels-0.14.0-cp38-cp38-macosx_11_0_arm64.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch<3.0.0,>=2.0.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pytorch-forecasting) (2.1.0)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from fastapi>=0.80->pytorch-forecasting)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from fastapi>=0.80->pytorch-forecasting) (2.4.2)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.80->pytorch-forecasting)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from fastapi>=0.80->pytorch-forecasting)\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0)\n",
      "Requirement already satisfied: fsspec<2025.0,>2021.06.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2023.10.0)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.24.4)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.1)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.1)\n",
      "Collecting pytorch-lightning (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
      "  Downloading pytorch_lightning-2.1.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading alembic-1.12.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading SQLAlchemy-2.0.23-cp38-cp38-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.2.0)\n",
      "Requirement already satisfied: filelock in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting) (5.2.0)\n",
      "Collecting patsy>=0.5.2 (from statsmodels->pytorch-forecasting)\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Mako (from alembic>=1.5.0->optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: importlib-metadata in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from alembic>=1.5.0->optuna<4.0.0,>=3.1.0->pytorch-forecasting) (6.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.80->pytorch-forecasting) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.80->pytorch-forecasting) (1.2.0)\n",
      "Collecting exceptiongroup (from anyio<4.0.0,>=3.7.1->fastapi>=0.80->pytorch-forecasting)\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: requests in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.8.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->pytorch-forecasting) (3.11.0)\n",
      "Requirement already satisfied: six in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from patsy>=0.5.2->statsmodels->pytorch-forecasting) (1.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.80->pytorch-forecasting) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.80->pytorch-forecasting) (2.10.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from jinja2->torch<3.0.0,>=2.0.0->pytorch-forecasting) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from sympy->torch<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2023.7.22)\n",
      "Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning-2.1.1-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_optimizer-2.12.0-py3-none-any.whl (155 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.8/155.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading SQLAlchemy-2.0.23-cp38-cp38-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading pytorch_lightning-2.1.1-py3-none-any.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, patsy, Mako, exceptiongroup, colorlog, sqlalchemy, lightning-utilities, anyio, torchmetrics, statsmodels, starlette, pytorch-optimizer, alembic, pytorch-lightning, optuna, fastapi, lightning, pytorch-forecasting\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Mako-1.3.0 alembic-1.12.1 anyio-3.7.1 colorlog-6.7.0 exceptiongroup-1.1.3 fastapi-0.104.1 lightning-2.1.1 lightning-utilities-0.9.0 optuna-3.4.0 patsy-0.5.3 pytorch-forecasting-1.0.0 pytorch-lightning-2.1.1 pytorch-optimizer-2.12.0 sqlalchemy-2.0.23 starlette-0.27.0 statsmodels-0.14.0 torchmetrics-1.2.0 typing-extensions-4.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7205202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor, IntTensor\n",
    "\n",
    "from pytorch_forecasting import metrics\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef1f5d",
   "metadata": {},
   "source": [
    "# WALMART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f8fce",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df45dee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421565</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>508.37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421566</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>628.10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421567</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>1061.02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421568</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>760.01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421569</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>1076.80</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421570 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  Weekly_Sales  IsHoliday\n",
       "0           1     1  2010-02-05      24924.50      False\n",
       "1           1     1  2010-02-12      46039.49       True\n",
       "2           1     1  2010-02-19      41595.55      False\n",
       "3           1     1  2010-02-26      19403.54      False\n",
       "4           1     1  2010-03-05      21827.90      False\n",
       "...       ...   ...         ...           ...        ...\n",
       "421565     45    98  2012-09-28        508.37      False\n",
       "421566     45    98  2012-10-05        628.10      False\n",
       "421567     45    98  2012-10-12       1061.02      False\n",
       "421568     45    98  2012-10-19        760.01      False\n",
       "421569     45    98  2012-10-26       1076.80      False\n",
       "\n",
       "[421570 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walmart_sales_train_df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "walmart_sales_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13415de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество тренировочных строк - 421570\n",
      "Количество колонок - 5\n",
      "\n",
      "Колонки:\n",
      "\n",
      "Store\n",
      "Dept\n",
      "Date\n",
      "Weekly_Sales\n",
      "IsHoliday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество тренировочных строк - {walmart_sales_train_df.shape[0]}\")\n",
    "print(f\"Количество колонок - {walmart_sales_train_df.shape[1]}\")\n",
    "print(f\"\\nКолонки:\\n\")\n",
    "print('\\n'.join(walmart_sales_train_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275487b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Store - номер магазина \n",
      "\n",
      "Dept - номер департамента магазина\n",
      "\n",
      "Date - дата\n",
      "\n",
      "Weekly_Sales - продажи за неделю(таргет)\n",
      "\n",
      "IsHoliday - выходной ли\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Store - номер магазина \\n\n",
    "Dept - номер департамента магазина\\n\n",
    "Date - дата\\n\n",
    "Weekly_Sales - продажи за неделю(таргет)\\n\n",
    "IsHoliday - выходной ли\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d24f27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.200546</td>\n",
       "      <td>44.260317</td>\n",
       "      <td>15981.258123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.785297</td>\n",
       "      <td>30.492054</td>\n",
       "      <td>22711.183519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4988.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2079.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7612.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>20205.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>693099.360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Store           Dept   Weekly_Sales\n",
       "count  421570.000000  421570.000000  421570.000000\n",
       "mean       22.200546      44.260317   15981.258123\n",
       "std        12.785297      30.492054   22711.183519\n",
       "min         1.000000       1.000000   -4988.940000\n",
       "25%        11.000000      18.000000    2079.650000\n",
       "50%        22.000000      37.000000    7612.030000\n",
       "75%        33.000000      74.000000   20205.852500\n",
       "max        45.000000      99.000000  693099.360000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walmart_sales_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc7b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Магазины могут быть - [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45]\n",
      "Длина - 45\n",
      "\n",
      "Департаменты могут быть - [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 58, 59, 60, 65, 67, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 85, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "Длина - 81\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Сount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>45</td>\n",
       "      <td>94</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>45</td>\n",
       "      <td>95</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>45</td>\n",
       "      <td>97</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3331 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store  Dept  Сount\n",
       "0         1     1    143\n",
       "1         1     2    143\n",
       "2         1     3    143\n",
       "3         1     4    143\n",
       "4         1     5    143\n",
       "...     ...   ...    ...\n",
       "3326     45    94    134\n",
       "3327     45    95    143\n",
       "3328     45    96      2\n",
       "3329     45    97    143\n",
       "3330     45    98    135\n",
       "\n",
       "[3331 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_store = walmart_sales_train_df.Store.unique()\n",
    "print(f\"Магазины могут быть - {unique_store}\")\n",
    "print(f\"Длина - {len(unique_store)}\\n\")\n",
    "\n",
    "unique_dept = sorted(list(walmart_sales_train_df.Dept.unique()))\n",
    "print(f\"Департаменты могут быть - {unique_dept}\")\n",
    "print(f\"Длина - {len(unique_dept)}\\n\")\n",
    "\n",
    "walmart_sales_train_df.groupby(['Store','Dept']).size().reset_index().rename(columns={0:'Сount'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc33cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48036b27",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12394f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115059</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115060</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115061</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115062</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115063</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115064 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  IsHoliday\n",
       "0           1     1  2012-11-02      False\n",
       "1           1     1  2012-11-09      False\n",
       "2           1     1  2012-11-16      False\n",
       "3           1     1  2012-11-23       True\n",
       "4           1     1  2012-11-30      False\n",
       "...       ...   ...         ...        ...\n",
       "115059     45    98  2013-06-28      False\n",
       "115060     45    98  2013-07-05      False\n",
       "115061     45    98  2013-07-12      False\n",
       "115062     45    98  2013-07-19      False\n",
       "115063     45    98  2013-07-26      False\n",
       "\n",
       "[115064 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walmart_sales_test_df = pd.read_csv(\"../data/raw/test.csv\")\n",
    "walmart_sales_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9614e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2013-07-26 00:00:00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dates = walmart_sales_test_df[(walmart_sales_test_df.Store == 1) & (walmart_sales_test_df.Dept == 1)].Date\n",
    "\n",
    "temp_dates = pd.to_datetime(temp_dates)\n",
    "temp_dates.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707f9c6",
   "metadata": {},
   "source": [
    "### FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24cded3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8185</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>76.05</td>\n",
       "      <td>3.639</td>\n",
       "      <td>4842.29</td>\n",
       "      <td>975.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2449.97</td>\n",
       "      <td>3169.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>77.50</td>\n",
       "      <td>3.614</td>\n",
       "      <td>9090.48</td>\n",
       "      <td>2268.58</td>\n",
       "      <td>582.74</td>\n",
       "      <td>5797.47</td>\n",
       "      <td>1514.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>79.37</td>\n",
       "      <td>3.614</td>\n",
       "      <td>3789.94</td>\n",
       "      <td>1827.31</td>\n",
       "      <td>85.72</td>\n",
       "      <td>744.84</td>\n",
       "      <td>2150.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>82.84</td>\n",
       "      <td>3.737</td>\n",
       "      <td>2961.49</td>\n",
       "      <td>1047.07</td>\n",
       "      <td>204.19</td>\n",
       "      <td>363.00</td>\n",
       "      <td>1059.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-26</td>\n",
       "      <td>76.06</td>\n",
       "      <td>3.804</td>\n",
       "      <td>212.02</td>\n",
       "      <td>851.73</td>\n",
       "      <td>2.06</td>\n",
       "      <td>10.88</td>\n",
       "      <td>1864.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8190 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
       "0         1  2010-02-05        42.31       2.572        NaN        NaN   \n",
       "1         1  2010-02-12        38.51       2.548        NaN        NaN   \n",
       "2         1  2010-02-19        39.93       2.514        NaN        NaN   \n",
       "3         1  2010-02-26        46.63       2.561        NaN        NaN   \n",
       "4         1  2010-03-05        46.50       2.625        NaN        NaN   \n",
       "...     ...         ...          ...         ...        ...        ...   \n",
       "8185     45  2013-06-28        76.05       3.639    4842.29     975.03   \n",
       "8186     45  2013-07-05        77.50       3.614    9090.48    2268.58   \n",
       "8187     45  2013-07-12        79.37       3.614    3789.94    1827.31   \n",
       "8188     45  2013-07-19        82.84       3.737    2961.49    1047.07   \n",
       "8189     45  2013-07-26        76.06       3.804     212.02     851.73   \n",
       "\n",
       "      MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
       "0           NaN        NaN        NaN  211.096358         8.106      False  \n",
       "1           NaN        NaN        NaN  211.242170         8.106       True  \n",
       "2           NaN        NaN        NaN  211.289143         8.106      False  \n",
       "3           NaN        NaN        NaN  211.319643         8.106      False  \n",
       "4           NaN        NaN        NaN  211.350143         8.106      False  \n",
       "...         ...        ...        ...         ...           ...        ...  \n",
       "8185       3.00    2449.97    3169.69         NaN           NaN      False  \n",
       "8186     582.74    5797.47    1514.93         NaN           NaN      False  \n",
       "8187      85.72     744.84    2150.36         NaN           NaN      False  \n",
       "8188     204.19     363.00    1059.46         NaN           NaN      False  \n",
       "8189       2.06      10.88    1864.57         NaN           NaN      False  \n",
       "\n",
       "[8190 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walmart_features_df = pd.read_csv(\"../data/raw/features.csv\")\n",
    "walmart_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b6182ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_features_df.Date = pd.to_datetime(walmart_features_df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b3b9139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2010-02-05    45\n",
      "2012-04-13    45\n",
      "2012-04-27    45\n",
      "2012-05-04    45\n",
      "2012-05-11    45\n",
      "2012-05-18    45\n",
      "2012-05-25    45\n",
      "2012-06-01    45\n",
      "2012-06-08    45\n",
      "2012-06-15    45\n",
      "2012-06-22    45\n",
      "2012-06-29    45\n",
      "2012-07-06    45\n",
      "2012-07-13    45\n",
      "2012-07-20    45\n",
      "2012-07-27    45\n",
      "2012-08-03    45\n",
      "2012-08-10    45\n",
      "2012-08-17    45\n",
      "2012-08-24    45\n",
      "2012-08-31    45\n",
      "2012-04-20    45\n",
      "2012-04-06    45\n",
      "2012-09-14    45\n",
      "2012-03-30    45\n",
      "2011-11-18    45\n",
      "2011-11-25    45\n",
      "2011-12-02    45\n",
      "2011-12-09    45\n",
      "2011-12-16    45\n",
      "2011-12-23    45\n",
      "2011-12-30    45\n",
      "2012-01-06    45\n",
      "2012-01-13    45\n",
      "2012-01-20    45\n",
      "2012-01-27    45\n",
      "2012-02-03    45\n",
      "2012-02-10    45\n",
      "2012-02-17    45\n",
      "2012-02-24    45\n",
      "2012-03-02    45\n",
      "2012-03-09    45\n",
      "2012-03-16    45\n",
      "2012-03-23    45\n",
      "2012-09-07    45\n",
      "2012-09-21    45\n",
      "2010-02-12    45\n",
      "2013-03-01    45\n",
      "2013-03-15    45\n",
      "2013-03-22    45\n",
      "2013-03-29    45\n",
      "2013-04-05    45\n",
      "2013-04-12    45\n",
      "2013-04-19    45\n",
      "2013-04-26    45\n",
      "2013-05-03    45\n",
      "2013-05-10    45\n",
      "2013-05-17    45\n",
      "2013-05-24    45\n",
      "2013-05-31    45\n",
      "2013-06-07    45\n",
      "2013-06-14    45\n",
      "2013-06-21    45\n",
      "2013-06-28    45\n",
      "2013-07-05    45\n",
      "2013-07-12    45\n",
      "2013-07-19    45\n",
      "2013-03-08    45\n",
      "2013-02-22    45\n",
      "2012-09-28    45\n",
      "2013-02-15    45\n",
      "2012-10-05    45\n",
      "2012-10-12    45\n",
      "2012-10-19    45\n",
      "2012-10-26    45\n",
      "2012-11-02    45\n",
      "2012-11-09    45\n",
      "2012-11-16    45\n",
      "2012-11-23    45\n",
      "2012-11-30    45\n",
      "2012-12-07    45\n",
      "2012-12-14    45\n",
      "2012-12-21    45\n",
      "2012-12-28    45\n",
      "2013-01-04    45\n",
      "2013-01-11    45\n",
      "2013-01-18    45\n",
      "2013-01-25    45\n",
      "2013-02-01    45\n",
      "2013-02-08    45\n",
      "2011-11-11    45\n",
      "2011-11-04    45\n",
      "2011-10-28    45\n",
      "2010-07-16    45\n",
      "2010-07-30    45\n",
      "2010-08-06    45\n",
      "2010-08-13    45\n",
      "2010-08-20    45\n",
      "2010-08-27    45\n",
      "2010-09-03    45\n",
      "2010-09-10    45\n",
      "2010-09-17    45\n",
      "2010-09-24    45\n",
      "2010-10-01    45\n",
      "2010-10-08    45\n",
      "2010-10-15    45\n",
      "2010-10-22    45\n",
      "2010-10-29    45\n",
      "2010-11-05    45\n",
      "2010-11-12    45\n",
      "2010-11-19    45\n",
      "2010-11-26    45\n",
      "2010-12-03    45\n",
      "2010-07-23    45\n",
      "2010-07-09    45\n",
      "2011-10-21    45\n",
      "2010-07-02    45\n",
      "2010-02-19    45\n",
      "2010-02-26    45\n",
      "2010-03-05    45\n",
      "2010-03-12    45\n",
      "2010-03-19    45\n",
      "2010-03-26    45\n",
      "2010-04-02    45\n",
      "2010-04-09    45\n",
      "2010-04-16    45\n",
      "2010-04-23    45\n",
      "2010-04-30    45\n",
      "2010-05-07    45\n",
      "2010-05-14    45\n",
      "2010-05-21    45\n",
      "2010-05-28    45\n",
      "2010-06-04    45\n",
      "2010-06-11    45\n",
      "2010-06-18    45\n",
      "2010-06-25    45\n",
      "2010-12-10    45\n",
      "2010-12-17    45\n",
      "2010-12-24    45\n",
      "2010-12-31    45\n",
      "2011-06-10    45\n",
      "2011-06-17    45\n",
      "2011-06-24    45\n",
      "2011-07-01    45\n",
      "2011-07-08    45\n",
      "2011-07-15    45\n",
      "2011-07-22    45\n",
      "2011-07-29    45\n",
      "2011-08-05    45\n",
      "2011-08-12    45\n",
      "2011-08-19    45\n",
      "2011-08-26    45\n",
      "2011-09-02    45\n",
      "2011-09-09    45\n",
      "2011-09-16    45\n",
      "2011-09-23    45\n",
      "2011-09-30    45\n",
      "2011-10-07    45\n",
      "2011-10-14    45\n",
      "2011-06-03    45\n",
      "2011-05-27    45\n",
      "2011-05-20    45\n",
      "2011-03-04    45\n",
      "2011-01-07    45\n",
      "2011-01-14    45\n",
      "2011-01-21    45\n",
      "2011-01-28    45\n",
      "2011-02-04    45\n",
      "2011-02-11    45\n",
      "2011-02-18    45\n",
      "2011-02-25    45\n",
      "2011-03-11    45\n",
      "2011-05-13    45\n",
      "2011-03-18    45\n",
      "2011-03-25    45\n",
      "2011-04-01    45\n",
      "2011-04-08    45\n",
      "2011-04-15    45\n",
      "2011-04-22    45\n",
      "2011-04-29    45\n",
      "2011-05-06    45\n",
      "2013-07-26    45\n"
     ]
    }
   ],
   "source": [
    "print(walmart_features_df.Date.value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f87cd1",
   "metadata": {},
   "source": [
    "### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c79c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>202307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>37392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>205863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>34875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>202505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B</td>\n",
       "      <td>70713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>155078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B</td>\n",
       "      <td>125833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>126512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>207499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>B</td>\n",
       "      <td>112238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>A</td>\n",
       "      <td>219622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>200898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>123737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>B</td>\n",
       "      <td>57197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>B</td>\n",
       "      <td>93188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>B</td>\n",
       "      <td>120653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>A</td>\n",
       "      <td>203819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>A</td>\n",
       "      <td>203742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>B</td>\n",
       "      <td>140167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>B</td>\n",
       "      <td>119557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>B</td>\n",
       "      <td>114533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>A</td>\n",
       "      <td>203819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>B</td>\n",
       "      <td>128107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>A</td>\n",
       "      <td>152513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>A</td>\n",
       "      <td>204184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>206302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>B</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>C</td>\n",
       "      <td>42988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>203750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>203007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>A</td>\n",
       "      <td>39690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>A</td>\n",
       "      <td>158114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>B</td>\n",
       "      <td>103681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>A</td>\n",
       "      <td>39910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>C</td>\n",
       "      <td>39910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>C</td>\n",
       "      <td>39690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>184109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>A</td>\n",
       "      <td>155083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>A</td>\n",
       "      <td>196321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>C</td>\n",
       "      <td>39690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>C</td>\n",
       "      <td>41062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>C</td>\n",
       "      <td>39910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Store Type    Size\n",
       "0       1    A  151315\n",
       "1       2    A  202307\n",
       "2       3    B   37392\n",
       "3       4    A  205863\n",
       "4       5    B   34875\n",
       "5       6    A  202505\n",
       "6       7    B   70713\n",
       "7       8    A  155078\n",
       "8       9    B  125833\n",
       "9      10    B  126512\n",
       "10     11    A  207499\n",
       "11     12    B  112238\n",
       "12     13    A  219622\n",
       "13     14    A  200898\n",
       "14     15    B  123737\n",
       "15     16    B   57197\n",
       "16     17    B   93188\n",
       "17     18    B  120653\n",
       "18     19    A  203819\n",
       "19     20    A  203742\n",
       "20     21    B  140167\n",
       "21     22    B  119557\n",
       "22     23    B  114533\n",
       "23     24    A  203819\n",
       "24     25    B  128107\n",
       "25     26    A  152513\n",
       "26     27    A  204184\n",
       "27     28    A  206302\n",
       "28     29    B   93638\n",
       "29     30    C   42988\n",
       "30     31    A  203750\n",
       "31     32    A  203007\n",
       "32     33    A   39690\n",
       "33     34    A  158114\n",
       "34     35    B  103681\n",
       "35     36    A   39910\n",
       "36     37    C   39910\n",
       "37     38    C   39690\n",
       "38     39    A  184109\n",
       "39     40    A  155083\n",
       "40     41    A  196321\n",
       "41     42    C   39690\n",
       "42     43    C   41062\n",
       "43     44    C   39910\n",
       "44     45    B  118221"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walmart_stores_df = pd.read_csv(\"../data/raw/stores.csv\")\n",
    "walmart_stores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b7381",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f87c160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(walmart_stores_df, pd.merge(walmart_sales_train_df, walmart_features_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f52a02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=[\"MarkDown1\", 'MarkDown2', 'MarkDown3', \"MarkDown4\", 'MarkDown5', 'Type', 'Fuel_Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47d8234b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Size</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>151315</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>50605.27</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>151315</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>13740.12</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>151315</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>39954.04</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>151315</td>\n",
       "      <td>5</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>32229.38</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421565</th>\n",
       "      <td>45</td>\n",
       "      <td>118221</td>\n",
       "      <td>93</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>2487.80</td>\n",
       "      <td>False</td>\n",
       "      <td>58.85</td>\n",
       "      <td>192.308899</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421566</th>\n",
       "      <td>45</td>\n",
       "      <td>118221</td>\n",
       "      <td>94</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>5203.31</td>\n",
       "      <td>False</td>\n",
       "      <td>58.85</td>\n",
       "      <td>192.308899</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421567</th>\n",
       "      <td>45</td>\n",
       "      <td>118221</td>\n",
       "      <td>95</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>56017.47</td>\n",
       "      <td>False</td>\n",
       "      <td>58.85</td>\n",
       "      <td>192.308899</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421568</th>\n",
       "      <td>45</td>\n",
       "      <td>118221</td>\n",
       "      <td>97</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>6817.48</td>\n",
       "      <td>False</td>\n",
       "      <td>58.85</td>\n",
       "      <td>192.308899</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421569</th>\n",
       "      <td>45</td>\n",
       "      <td>118221</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>1076.80</td>\n",
       "      <td>False</td>\n",
       "      <td>58.85</td>\n",
       "      <td>192.308899</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421570 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store    Size  Dept        Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
       "0           1  151315     1  2010-02-05      24924.50      False        42.31   \n",
       "1           1  151315     2  2010-02-05      50605.27      False        42.31   \n",
       "2           1  151315     3  2010-02-05      13740.12      False        42.31   \n",
       "3           1  151315     4  2010-02-05      39954.04      False        42.31   \n",
       "4           1  151315     5  2010-02-05      32229.38      False        42.31   \n",
       "...       ...     ...   ...         ...           ...        ...          ...   \n",
       "421565     45  118221    93  2012-10-26       2487.80      False        58.85   \n",
       "421566     45  118221    94  2012-10-26       5203.31      False        58.85   \n",
       "421567     45  118221    95  2012-10-26      56017.47      False        58.85   \n",
       "421568     45  118221    97  2012-10-26       6817.48      False        58.85   \n",
       "421569     45  118221    98  2012-10-26       1076.80      False        58.85   \n",
       "\n",
       "               CPI  Unemployment  \n",
       "0       211.096358         8.106  \n",
       "1       211.096358         8.106  \n",
       "2       211.096358         8.106  \n",
       "3       211.096358         8.106  \n",
       "4       211.096358         8.106  \n",
       "...            ...           ...  \n",
       "421565  192.308899         8.667  \n",
       "421566  192.308899         8.667  \n",
       "421567  192.308899         8.667  \n",
       "421568  192.308899         8.667  \n",
       "421569  192.308899         8.667  \n",
       "\n",
       "[421570 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67982f2",
   "metadata": {},
   "source": [
    "#### Preprocessing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2af841f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.Date = pd.to_datetime(X.Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc805513",
   "metadata": {},
   "source": [
    "#### Sorting X by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90cb02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_values(by=['Date', 'Store'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b2323",
   "metadata": {},
   "source": [
    "#### Replacing date with number of a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1780687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Week'] = X.Date.apply(lambda x: x.isocalendar()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f09174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a104a",
   "metadata": {},
   "source": [
    "#### Splitting dataset to train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aed23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_len = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7a414b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.1\n",
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c0d38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(X_len * test_ratio)\n",
    "val_size = int(X_len * val_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9b0c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X.iloc[X_len-test_size:]\n",
    "X_val = X.iloc[X_len-test_size-val_size:X_len-test_size]\n",
    "X_train = X.iloc[:X_len-test_size-val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f2209",
   "metadata": {},
   "source": [
    "#### Preprocessing Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01aa8d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_label_encoder = LabelEncoder()\n",
    "store_label_encoder.fit(sorted(list(X_train.Store.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ea3cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3350660351.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Store'] = store_label_encoder.transform(X_train[['Store']])\n",
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3350660351.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val['Store'] = store_label_encoder.transform(X_val[['Store']])\n",
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3350660351.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['Store'] = store_label_encoder.transform(X_test[['Store']])\n"
     ]
    }
   ],
   "source": [
    "X_train['Store'] = store_label_encoder.transform(X_train[['Store']])\n",
    "X_val['Store'] = store_label_encoder.transform(X_val[['Store']])\n",
    "X_test['Store'] = store_label_encoder.transform(X_test[['Store']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22026cd",
   "metadata": {},
   "source": [
    "#### Preprocessing Dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fd81f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_label_encoder = LabelEncoder()\n",
    "dept_label_encoder.fit(sorted(list(X_train.Dept.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0c5f214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2324919871.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Dept'] = dept_label_encoder.transform(X_train[['Dept']])\n",
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2324919871.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val['Dept'] = dept_label_encoder.transform(X_val[['Dept']])\n",
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2324919871.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['Dept'] = dept_label_encoder.transform(X_test[['Dept']])\n"
     ]
    }
   ],
   "source": [
    "X_train['Dept'] = dept_label_encoder.transform(X_train[['Dept']])\n",
    "X_val['Dept'] = dept_label_encoder.transform(X_val[['Dept']])\n",
    "X_test['Dept'] = dept_label_encoder.transform(X_test[['Dept']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825acaf0",
   "metadata": {},
   "source": [
    "#### Preprocessing Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "911ff443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_min_max_scaler = MinMaxScaler()\n",
    "size_min_max_scaler.fit(X_train[['Size']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5af6eff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2702060106.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.Size = size_min_max_scaler.transform(X_train[['Size']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2702060106.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val.Size = size_min_max_scaler.transform(X_val[['Size']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2702060106.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.Size = size_min_max_scaler.transform(X_test[['Size']])\n"
     ]
    }
   ],
   "source": [
    "X_train.Size = size_min_max_scaler.transform(X_train[['Size']])\n",
    "X_val.Size = size_min_max_scaler.transform(X_val[['Size']])\n",
    "X_test.Size = size_min_max_scaler.transform(X_test[['Size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a1fc1",
   "metadata": {},
   "source": [
    "#### Preprocessing IsHoliday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e91cffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_holiday_transformer = lambda x: 1 if x else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22cb4bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3890250643.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.IsHoliday = X_train.IsHoliday.apply(is_holiday_transformer)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3890250643.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val.IsHoliday = X_val.IsHoliday.apply(is_holiday_transformer)\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3890250643.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.IsHoliday = X_test.IsHoliday.apply(is_holiday_transformer)\n"
     ]
    }
   ],
   "source": [
    "X_train.IsHoliday = X_train.IsHoliday.apply(is_holiday_transformer)\n",
    "X_val.IsHoliday = X_val.IsHoliday.apply(is_holiday_transformer)\n",
    "X_test.IsHoliday = X_test.IsHoliday.apply(is_holiday_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee1169",
   "metadata": {},
   "source": [
    "#### Preprocessing Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "995e8e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_min_max_scaler = MinMaxScaler()\n",
    "temperature_min_max_scaler.fit(X_train[['Temperature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd5f7c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/1389029567.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.Temperature = temperature_min_max_scaler.transform(X_train[['Temperature']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/1389029567.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val.Temperature = temperature_min_max_scaler.transform(X_val[['Temperature']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/1389029567.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.Temperature = temperature_min_max_scaler.transform(X_test[['Temperature']])\n"
     ]
    }
   ],
   "source": [
    "X_train.Temperature = temperature_min_max_scaler.transform(X_train[['Temperature']])\n",
    "X_val.Temperature = temperature_min_max_scaler.transform(X_val[['Temperature']])\n",
    "X_test.Temperature = temperature_min_max_scaler.transform(X_test[['Temperature']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc000d3d",
   "metadata": {},
   "source": [
    "#### Preprocessing CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcc4e9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpi_min_max_scaler = MinMaxScaler()\n",
    "cpi_min_max_scaler.fit(X_train[['CPI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80b7a294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2361466852.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.CPI = cpi_min_max_scaler.transform(X_train[['CPI']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2361466852.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val.CPI = cpi_min_max_scaler.transform(X_val[['CPI']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/2361466852.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.CPI = cpi_min_max_scaler.transform(X_test[['CPI']])\n"
     ]
    }
   ],
   "source": [
    "X_train.CPI = cpi_min_max_scaler.transform(X_train[['CPI']])\n",
    "X_val.CPI = cpi_min_max_scaler.transform(X_val[['CPI']])\n",
    "X_test.CPI = cpi_min_max_scaler.transform(X_test[['CPI']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c9618",
   "metadata": {},
   "source": [
    "#### Preprocessing Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcd21019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unemployment_min_max_scaler = MinMaxScaler()\n",
    "unemployment_min_max_scaler.fit(X_train[['Unemployment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef15ae36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/1471874588.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.Unemployment = unemployment_min_max_scaler.transform(X_train[['Unemployment']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/1471874588.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val.Unemployment = unemployment_min_max_scaler.transform(X_val[['Unemployment']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/1471874588.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.Unemployment = unemployment_min_max_scaler.transform(X_test[['Unemployment']])\n"
     ]
    }
   ],
   "source": [
    "X_train.Unemployment = unemployment_min_max_scaler.transform(X_train[['Unemployment']])\n",
    "X_val.Unemployment = unemployment_min_max_scaler.transform(X_val[['Unemployment']])\n",
    "X_test.Unemployment = unemployment_min_max_scaler.transform(X_test[['Unemployment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3b629",
   "metadata": {},
   "source": [
    "#### Preprocessing Weekly_Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "388d8812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuantileTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuantileTransformer</label><div class=\"sk-toggleable__content\"><pre>QuantileTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "QuantileTransformer()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_sales_quantile_transformer = QuantileTransformer()\n",
    "weekly_sales_quantile_transformer.fit(X_train[['Weekly_Sales']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96dca01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3902564762.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.Weekly_Sales = weekly_sales_quantile_transformer.transform(X_train[['Weekly_Sales']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3902564762.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val.Weekly_Sales = weekly_sales_quantile_transformer.transform(X_val[['Weekly_Sales']])\n",
      "/var/folders/7x/qzp6r7jn19gf5g0r68gpc_3r0000gp/T/ipykernel_41431/3902564762.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.Weekly_Sales = weekly_sales_quantile_transformer.transform(X_test[['Weekly_Sales']])\n"
     ]
    }
   ],
   "source": [
    "X_train.Weekly_Sales = weekly_sales_quantile_transformer.transform(X_train[['Weekly_Sales']])\n",
    "X_val.Weekly_Sales = weekly_sales_quantile_transformer.transform(X_val[['Weekly_Sales']])\n",
    "X_test.Weekly_Sales = weekly_sales_quantile_transformer.transform(X_test[['Weekly_Sales']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b88df",
   "metadata": {},
   "source": [
    "### Making Temporary Store Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fa6d1",
   "metadata": {},
   "source": [
    "Store[1...45]:\n",
    "    dept\n",
    "    size\n",
    "    date - list\n",
    "    weekly - list\n",
    "    is_holiday - list\n",
    "    temperature - list\n",
    "    CPI - list\n",
    "    Unemployment - list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a1bed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreSales:\n",
    "    def __init__(self, store, dept, size, week, weekly_sales, is_holiday, temperature, cpi, unemployment, indices_window_size, indices_step_size):\n",
    "        self.store = store\n",
    "        self.dept = dept\n",
    "        self.size = size\n",
    "        \n",
    "        self.week = week\n",
    "        self.week_sin = self.sin_transform(self.week)\n",
    "        self.week_cos = self.cos_transform(self.week)\n",
    "        \n",
    "        self.weekly_sales = weekly_sales\n",
    "        self.is_holiday = is_holiday\n",
    "        self.temperature = temperature\n",
    "        self.cpi = cpi\n",
    "        self.unemployment = unemployment\n",
    "        \n",
    "        self.indices = self.get_indices(window_size=indices_window_size, step_size=indices_step_size)\n",
    "    \n",
    "    def get_indices(self, window_size, step_size):\n",
    "        \n",
    "        stop_position = len(self.weekly_sales) - 1\n",
    "        \n",
    "        subseq_first_idx = 0\n",
    "        \n",
    "        subseq_last_idx = window_size\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        while subseq_last_idx <= stop_position:\n",
    "\n",
    "            indices.append((subseq_first_idx, subseq_last_idx))\n",
    "            \n",
    "            subseq_first_idx += step_size\n",
    "            \n",
    "            subseq_last_idx += step_size\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def get_sequence(self, index):\n",
    "        tabular_categorical_features = (self.store, self.dept)\n",
    "        tabular_numerical_features = (self.size)\n",
    "        tabular_features = [tabular_categorical_features, tabular_numerical_features]\n",
    "\n",
    "        start_idx = self.indices[index][0]\n",
    "        end_idx = self.indices[index][1]\n",
    "        \n",
    "        weeks = self.week[start_idx:end_idx]\n",
    "        weeks_sin = self.week_sin[start_idx:end_idx]\n",
    "        weeks_cos = self.week_cos[start_idx:end_idx]\n",
    "        \n",
    "        time_series = self.weekly_sales[start_idx:end_idx]\n",
    "        \n",
    "        holidays = self.is_holiday[start_idx:end_idx]\n",
    "        temperatures = self.temperature[start_idx:end_idx]\n",
    "        cpis = self.cpi[start_idx:end_idx]\n",
    "        unemployments = self.unemployment[start_idx:end_idx]\n",
    "        \n",
    "        time_series_features = [((holidays[i]), (time_series[i], weeks_sin[i], weeks_cos[i], temperatures[i], cpis[i], unemployments[i])) for i in range(len(time_series))]\n",
    "        \n",
    "        return (tabular_features, time_series_features)\n",
    "    \n",
    "    def sin_transform(self, values):\n",
    "        values = np.array(values)\n",
    "        return np.sin(2 * np.pi * values / 52)\n",
    "\n",
    "    def cos_transform(self, values):\n",
    "        values = np.array(values)\n",
    "        return np.cos(2 * np.pi * values / 52)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6588d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp_store_sales(df):\n",
    "    stores_sales = []\n",
    "    store_nums = sorted(list(X_train.Store.unique()))\n",
    "    for store_num in store_nums:\n",
    "        df_store = df[df.Store == store_num]\n",
    "    \n",
    "        dept_nums = sorted(list(df_store.Dept.unique()))\n",
    "    \n",
    "        for dept_num in dept_nums:\n",
    "            df_store_dept = df_store[df_store.Dept == dept_num]\n",
    "        \n",
    "            store_dept_sales = StoreSales(\n",
    "                store=store_num,\n",
    "                dept=dept_num, \n",
    "                size=float(df_store_dept.Size.unique()[0]), \n",
    "                week=df_store_dept.Week.tolist(), \n",
    "                weekly_sales=df_store_dept.Weekly_Sales.tolist(), \n",
    "                is_holiday=df_store_dept.IsHoliday.tolist(), \n",
    "                temperature=df_store_dept.Temperature.tolist(), \n",
    "                cpi=df_store_dept.CPI.tolist(), \n",
    "                unemployment=df_store_dept.Unemployment.tolist(), \n",
    "                indices_window_size=14, \n",
    "                indices_step_size=1)\n",
    "        \n",
    "            stores_sales.append(store_dept_sales)\n",
    "    return stores_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "648d5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store_sales = get_temp_store_sales(X_train)\n",
    "val_store_sales = get_temp_store_sales(X_val)\n",
    "test_store_sales = get_temp_store_sales(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5c9b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequences(store_sales):\n",
    "    subsequences = []\n",
    "    for store_sale in store_sales:\n",
    "        for subsequence_i in range(len(store_sale.indices)):\n",
    "            subsequent = store_sale.get_sequence(subsequence_i)\n",
    "            subsequences.append(subsequent)\n",
    "    return subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f308386",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = get_subsequences(train_store_sales)\n",
    "val_sequences = get_subsequences(val_store_sales)\n",
    "test_sequences = get_subsequences(test_store_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6566fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (0.7949050450975976,\n",
       "   0.5680647467311558,\n",
       "   0.8229838658936564,\n",
       "   0.43414872798434445,\n",
       "   0.8732979481134047,\n",
       "   0.37258667744870105)),\n",
       " (1,\n",
       "  (0.9106288561724998,\n",
       "   0.6631226582407952,\n",
       "   0.7485107481711011,\n",
       "   0.3969667318982387,\n",
       "   0.8747954600463519,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.8931248462908492,\n",
       "   0.7485107481711011,\n",
       "   0.6631226582407953,\n",
       "   0.41086105675146767,\n",
       "   0.8752778824172571,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.7366687245616126,\n",
       "   0.8229838658936564,\n",
       "   0.5680647467311558,\n",
       "   0.4764187866927593,\n",
       "   0.8755911230233195,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.7665001357254528,\n",
       "   0.8854560256532098,\n",
       "   0.4647231720437686,\n",
       "   0.47514677103718195,\n",
       "   0.8759043636293815,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.7577648113620016,\n",
       "   0.9350162426854148,\n",
       "   0.35460488704253557,\n",
       "   0.5856164383561644,\n",
       "   0.8762176042354435,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.7698973274961819,\n",
       "   0.970941817426052,\n",
       "   0.23931566428755804,\n",
       "   0.5542074363992172,\n",
       "   0.8745229427731145,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.8041064727680676,\n",
       "   0.992708874098054,\n",
       "   0.120536680255323,\n",
       "   0.5235812133072407,\n",
       "   0.8724936304525448,\n",
       "   0.37258667744870105)),\n",
       " (0,\n",
       "  (0.9405788585291074,\n",
       "   1.0,\n",
       "   -1.6081226496766366e-16,\n",
       "   0.6294520547945206,\n",
       "   0.8704643191589927,\n",
       "   0.34246436874557756)),\n",
       " (0,\n",
       "  (0.8980427354311082,\n",
       "   0.992708874098054,\n",
       "   -0.12053668025532288,\n",
       "   0.6645792563600782,\n",
       "   0.8684350078654406,\n",
       "   0.34246436874557756)),\n",
       " (0,\n",
       "  (0.7113223512732897,\n",
       "   0.9709418174260521,\n",
       "   -0.2393156642875575,\n",
       "   0.6690802348336594,\n",
       "   0.8670571867085979,\n",
       "   0.34246436874557756)),\n",
       " (0,\n",
       "  (0.6861181884615569,\n",
       "   0.9350162426854148,\n",
       "   -0.35460488704253545,\n",
       "   0.6545988258317026,\n",
       "   0.8665480197520448,\n",
       "   0.34246436874557756)),\n",
       " (0,\n",
       "  (0.6930410938867775,\n",
       "   0.8854560256532099,\n",
       "   -0.4647231720437685,\n",
       "   0.6797455968688845,\n",
       "   0.8660388527954921,\n",
       "   0.34246436874557756)),\n",
       " (0,\n",
       "  (0.7082272844860615,\n",
       "   0.8229838658936565,\n",
       "   -0.5680647467311557,\n",
       "   0.7300391389432485,\n",
       "   0.865529685838939,\n",
       "   0.34246436874557756))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afd8b3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250840"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1987d9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41657"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c15dd466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3add6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8684c",
   "metadata": {},
   "source": [
    "## Making Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc392fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalmartDataset(Dataset):\n",
    "    def __init__(self, data, enc_seq_len: int, dec_seq_len: int, target_seq_len: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data # (tabular_features,time_series_features)\n",
    "        \n",
    "        self.enc_seq_len = enc_seq_len\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        \n",
    "        tabular_features, time_series_features = item\n",
    "        src_time_series_categorical_features, src_time_series_numerical_features, trg_time_series_categorical_features, trg_time_series_numerical_features, trg_y = self.get_src_trg(time_series_features)\n",
    "        \n",
    "        tabular_categorical_features, tabular_numerical_features = tabular_features\n",
    "        tabular_categorical_features, tabular_numerical_features = list(tabular_categorical_features), [tabular_numerical_features]\n",
    "        \n",
    "        return {\n",
    "            'tabular_categorical_features': torch.IntTensor(tabular_categorical_features),\n",
    "            'tabular_numerical_features': torch.Tensor(tabular_numerical_features),\n",
    "            'src_time_series_categorical_features': torch.IntTensor(src_time_series_categorical_features),\n",
    "            'src_time_series_numerical_features': torch.Tensor(src_time_series_numerical_features),\n",
    "            'trg_time_series_categorical_features': torch.IntTensor(trg_time_series_categorical_features),\n",
    "            'trg_time_series_numerical_features': torch.Tensor(trg_time_series_numerical_features),\n",
    "            'trg_y': torch.Tensor(trg_y)\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def get_src_trg(\n",
    "            self,\n",
    "            time_series_features\n",
    "    ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "        #assert len(time_series_features) == self.enc_seq_len + self.target_seq_len, \"Features length does not equal (input length + target length)\"\n",
    "        \n",
    "        time_series_categorical_features = []\n",
    "        time_series_numerical_features = []\n",
    "        for t in range(len(time_series_features)):\n",
    "            time_step = time_series_features[t]\n",
    "            time_series_categorical_features.append(time_step[0])\n",
    "            time_series_numerical_features.append(list(time_step[1]))\n",
    "        \n",
    "        src_time_series_categorical_features = time_series_categorical_features[:self.enc_seq_len]\n",
    "        src_time_series_numerical_features = time_series_numerical_features[:self.enc_seq_len]\n",
    "        \n",
    "        trg_time_series_categorical_features = time_series_categorical_features[self.enc_seq_len - 1:self.enc_seq_len - 1 + self.dec_seq_len]\n",
    "        trg_time_series_numerical_features = time_series_numerical_features[self.enc_seq_len - 1:self.enc_seq_len - 1 + self.dec_seq_len]\n",
    "\n",
    "        #assert len(trg_time_series_numerical_features) == self.dec_seq_len, \"Length of trg num does not match target sequence length\"\n",
    "        #assert len(trg_time_series_categorical_features) == self.dec_seq_len, \"Length of trg cat does not match target sequence length\"\n",
    "\n",
    "        \n",
    "        trg_y_numerical_features = time_series_numerical_features[-self.target_seq_len:]\n",
    "        \n",
    "        trg_y = [trg_y_numerical_features[-1][0]]\n",
    "        \n",
    "        return src_time_series_categorical_features, src_time_series_numerical_features, trg_time_series_categorical_features, trg_time_series_numerical_features, trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fffa10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_seq_len = 10\n",
    "dec_seq_len = 4\n",
    "target_seq_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0abf4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WalmartDataset(\n",
    "    data=train_sequences, \n",
    "    enc_seq_len=enc_seq_len, \n",
    "    dec_seq_len=dec_seq_len, \n",
    "    target_seq_len=target_seq_len\n",
    ")\n",
    "\n",
    "val_dataset = WalmartDataset(\n",
    "    data=val_sequences, \n",
    "    enc_seq_len=enc_seq_len, \n",
    "    dec_seq_len=dec_seq_len, \n",
    "    target_seq_len=target_seq_len\n",
    ")\n",
    "\n",
    "test_dataset = WalmartDataset(\n",
    "    data=test_sequences, \n",
    "    enc_seq_len=enc_seq_len, \n",
    "    dec_seq_len=dec_seq_len, \n",
    "    target_seq_len=target_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32429426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tabular_categorical_features': tensor([0, 0], dtype=torch.int32),\n",
       " 'tabular_numerical_features': tensor([0.6303]),\n",
       " 'src_time_series_categorical_features': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'src_time_series_numerical_features': tensor([[ 7.9491e-01,  5.6806e-01,  8.2298e-01,  4.3415e-01,  8.7330e-01,\n",
       "           3.7259e-01],\n",
       "         [ 9.1063e-01,  6.6312e-01,  7.4851e-01,  3.9697e-01,  8.7480e-01,\n",
       "           3.7259e-01],\n",
       "         [ 8.9312e-01,  7.4851e-01,  6.6312e-01,  4.1086e-01,  8.7528e-01,\n",
       "           3.7259e-01],\n",
       "         [ 7.3667e-01,  8.2298e-01,  5.6806e-01,  4.7642e-01,  8.7559e-01,\n",
       "           3.7259e-01],\n",
       "         [ 7.6650e-01,  8.8546e-01,  4.6472e-01,  4.7515e-01,  8.7590e-01,\n",
       "           3.7259e-01],\n",
       "         [ 7.5776e-01,  9.3502e-01,  3.5460e-01,  5.8562e-01,  8.7622e-01,\n",
       "           3.7259e-01],\n",
       "         [ 7.6990e-01,  9.7094e-01,  2.3932e-01,  5.5421e-01,  8.7452e-01,\n",
       "           3.7259e-01],\n",
       "         [ 8.0411e-01,  9.9271e-01,  1.2054e-01,  5.2358e-01,  8.7249e-01,\n",
       "           3.7259e-01],\n",
       "         [ 9.4058e-01,  1.0000e+00, -1.6081e-16,  6.2945e-01,  8.7046e-01,\n",
       "           3.4246e-01],\n",
       "         [ 8.9804e-01,  9.9271e-01, -1.2054e-01,  6.6458e-01,  8.6844e-01,\n",
       "           3.4246e-01]]),\n",
       " 'trg_time_series_categorical_features': tensor([0, 0, 0, 0], dtype=torch.int32),\n",
       " 'trg_time_series_numerical_features': tensor([[ 0.8980,  0.9927, -0.1205,  0.6646,  0.8684,  0.3425],\n",
       "         [ 0.7113,  0.9709, -0.2393,  0.6691,  0.8671,  0.3425],\n",
       "         [ 0.6861,  0.9350, -0.3546,  0.6546,  0.8665,  0.3425],\n",
       "         [ 0.6930,  0.8855, -0.4647,  0.6797,  0.8660,  0.3425]]),\n",
       " 'trg_y': tensor([0.7082])}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13bfc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor, IntTensor\n",
    "\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            output_size=32\n",
    "    ):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tabular_cat_features_size: int,  # 2\n",
    "            tabular_cat_features_possible_nums: IntTensor,  # [45, 81]\n",
    "            tabular_cat_features_embeddings_dim: int,  # 32\n",
    "\n",
    "            tabular_num_features_size: int,  # 1\n",
    "            tabular_num_features_ffn_hidden_size: int,  # 32\n",
    "            tabular_num_features_ffn_output_dim: int,  # 22\n",
    "\n",
    "            time_series_cat_features_size: int,  # 1\n",
    "            time_series_cat_features_possible_nums: IntTensor,  # [2]\n",
    "            time_series_cat_features_embeddings_dim: int,  # 16\n",
    "\n",
    "            time_series_size: int,  # 10\n",
    "\n",
    "            d_model: int = 512,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tabular_cat_features_embeddings_table = torch.nn.ModuleList(\n",
    "            [torch.nn.Embedding(num_embeddings=tabular_cat_features_possible_nums[cat_feature_i],\n",
    "                                embedding_dim=tabular_cat_features_embeddings_dim)\n",
    "             for cat_feature_i in range(tabular_cat_features_size)]\n",
    "        )\n",
    "\n",
    "        self.tabular_cat_features_pos_embeddings_table = nn.Embedding(num_embeddings=tabular_cat_features_size + 1,\n",
    "                                                                      embedding_dim=tabular_cat_features_embeddings_dim)\n",
    "\n",
    "        self.tabular_num_features_ffn = FeedForwardLayer(input_size=tabular_num_features_size,\n",
    "                                                         hidden_size=tabular_num_features_ffn_hidden_size,\n",
    "                                                         output_size=tabular_num_features_ffn_output_dim)\n",
    "\n",
    "        self.time_series_cat_features_embeddings_table = nn.ModuleList(\n",
    "            [nn.Embedding(num_embeddings=time_series_cat_features_possible_nums[cat_feature_i],\n",
    "                          embedding_dim=time_series_cat_features_embeddings_dim)\n",
    "             for cat_feature_i in range(time_series_cat_features_size)]\n",
    "        )\n",
    "\n",
    "        self.tabular_cat_features_size = tabular_cat_features_size\n",
    "        self.tabular_num_features_size = tabular_num_features_size\n",
    "        self.tabular_features_size = tabular_cat_features_size + tabular_num_features_size\n",
    "\n",
    "        self.time_series_cat_features_size = time_series_cat_features_size\n",
    "        self.time_series_size = time_series_size\n",
    "\n",
    "        self.column_embedding_table = torch.nn.Embedding(num_embeddings=self.tabular_features_size + 1,\n",
    "                                                         embedding_dim=tabular_cat_features_embeddings_dim)\n",
    "        self.positional_embedding_table = torch.nn.Embedding(num_embeddings=self.time_series_size + 1,\n",
    "                                                             embedding_dim=tabular_cat_features_embeddings_dim)\n",
    "\n",
    "        self.linear_layer = nn.Linear(\n",
    "            in_features=self.time_series_size * tabular_num_features_ffn_output_dim + self.tabular_cat_features_size * tabular_cat_features_embeddings_dim + tabular_num_features_size * tabular_num_features_ffn_output_dim,\n",
    "            out_features=d_model\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            tabular_cat_features: Tensor,\n",
    "            tabular_num_features: Tensor,\n",
    "            time_series_cat_features: Tensor,\n",
    "            time_series_num_features: Tensor\n",
    "    ):\n",
    "        batch_size = tabular_cat_features.shape[0]\n",
    "\n",
    "        tabular_cat_features_embeddings = torch.stack(\n",
    "            [self.tabular_cat_features_embeddings_table[cat_feature_i](tabular_cat_features[:, cat_feature_i].long())\n",
    "             for cat_feature_i in range(tabular_cat_features.shape[1])], dim=0)\n",
    "\n",
    "        tabular_num_features = tabular_num_features.reshape(self.tabular_num_features_size, batch_size, -1)\n",
    "        tabular_num_features_embeddings = self.tabular_num_features_ffn(tabular_num_features)\n",
    "\n",
    "        time_series_cat_features = time_series_cat_features.reshape(self.time_series_size, batch_size, -1)\n",
    "        time_series_cat_features_embeddings = torch.cat([self.time_series_cat_features_embeddings_table[cat_feature_i](\n",
    "            time_series_cat_features[:, :, cat_feature_i].long())\n",
    "            for cat_feature_i in\n",
    "            range(time_series_cat_features.shape[-1])], dim=-1)\n",
    "\n",
    "        time_series_num_features = time_series_num_features.reshape(self.time_series_size, batch_size, -1)\n",
    "\n",
    "        tabular_embeddings = torch.cat([tabular_cat_features_embeddings, tabular_num_features_embeddings], dim=0)\n",
    "        time_series_embeddings = torch.cat([time_series_cat_features_embeddings, time_series_num_features], dim=-1)\n",
    "        \n",
    "        time_series_embeddings += self.column_embedding_table(torch.IntTensor([0]))\n",
    "        for step_i in range(len(time_series_embeddings)):\n",
    "            time_series_embeddings[step_i] += self.positional_embedding_table(torch.IntTensor([step_i + 1]))\n",
    "\n",
    "        tabular_embeddings += self.positional_embedding_table(torch.IntTensor([0]))\n",
    "        for column_i in range(len(tabular_embeddings)):\n",
    "            tabular_embeddings[column_i] += self.column_embedding_table(torch.IntTensor([column_i + 1]))\n",
    "\n",
    "        time_series_embeddings = time_series_embeddings.reshape(batch_size, self.time_series_size, -1)\n",
    "        tabular_embeddings = tabular_embeddings.reshape(batch_size, self.tabular_features_size, -1)\n",
    "\n",
    "        return self.linear_layer(torch.cat([tabular_embeddings, time_series_embeddings], dim=1).reshape(batch_size, -1))\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a transformer model that can be used for times series\n",
    "    forecasting. This time series transformer model is based on the paper by\n",
    "    Wu et al (2020) [1]. The paper will be referred to as \"the paper\".\n",
    "\n",
    "    A detailed description of the code can be found in my article here:\n",
    "\n",
    "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "\n",
    "    In cases where the paper does not specify what value was used for a specific\n",
    "    configuration/hyperparameter, this class uses the values from Vaswani et al\n",
    "    (2017) [2] or from PyTorch source code.\n",
    "\n",
    "    Unlike the paper, this class assumes that input layers, positional encoding\n",
    "    layers and linear mapping layers are separate from the encoder and decoder,\n",
    "    i.e. the encoder and decoder only do what is depicted as their sub-layers\n",
    "    in the paper. For practical purposes, this assumption does not make a\n",
    "    difference - it merely means that the linear and positional encoding layers\n",
    "    are implemented inside the present class and not inside the\n",
    "    Encoder() and Decoder() classes.\n",
    "\n",
    "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020).\n",
    "    'Deep Transformer Models for Time Series Forecasting:\n",
    "    The Influenza Prevalence Case'.\n",
    "    arXiv:2001.08317 [cs, stat] [Preprint].\n",
    "    Available at: http://arxiv.org/abs/2001.08317 (Accessed: 9 March 2022).\n",
    "\n",
    "    [2] Vaswani, A. et al. (2017)\n",
    "    'Attention Is All You Need'.\n",
    "    arXiv:1706.03762 [cs] [Preprint].\n",
    "    Available at: http://arxiv.org/abs/1706.03762 (Accessed: 9 March 2022).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tabular_cat_features_size: int,  # 2\n",
    "                 tabular_cat_features_possible_nums: IntTensor,  # [45, 81]\n",
    "                 tabular_cat_features_embeddings_dim: int,  # 32\n",
    "\n",
    "                 tabular_num_features_size: int,  # 1\n",
    "                 tabular_num_features_ffn_hidden_size: int,  # 32\n",
    "                 tabular_num_features_ffn_output_dim: int,  # 22\n",
    "\n",
    "                 time_series_cat_features_size: int,  # 1\n",
    "                 time_series_cat_features_possible_nums: IntTensor,  # [2]\n",
    "                 time_series_cat_features_embeddings_dim: int,  # 16\n",
    "\n",
    "                 enc_seq_len: int,  # 10\n",
    "\n",
    "                 dec_seq_len: int = 4,\n",
    "                 batch_first: bool = True,\n",
    "                 out_seq_len: int = 1,\n",
    "                 dim_val: int = 512,\n",
    "                 n_encoder_layers: int = 4,\n",
    "                 n_decoder_layers: int = 4,\n",
    "                 n_heads: int = 8,\n",
    "                 dropout_encoder: float = 0.2,\n",
    "                 dropout_decoder: float = 0.2,\n",
    "                 dropout_pos_enc: float = 0.1,\n",
    "                 dim_feedforward_encoder: int = 2048,\n",
    "                 dim_feedforward_decoder: int = 2048,\n",
    "                 num_predicted_features: int = 1\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce\n",
    "                     outputs of dimension dim_val\n",
    "\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer\n",
    "                                     of the encoder\n",
    "\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer\n",
    "                                     of the decoder\n",
    "\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 because we're\n",
    "                                    only forecasting FCR-N prices in DK2, but in\n",
    "                                    we wanted to also predict FCR-D with the same\n",
    "                                    model, num_predicted_features should be 2.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        # print(\"input_size is: {}\".format(input_size))\n",
    "        # print(\"dim_val is: {}\".format(dim_val))\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_embedding_layer = EmbeddingLayer(\n",
    "            tabular_cat_features_size=tabular_cat_features_size,\n",
    "            tabular_cat_features_possible_nums=tabular_cat_features_possible_nums,\n",
    "            tabular_cat_features_embeddings_dim=tabular_cat_features_embeddings_dim,\n",
    "            tabular_num_features_size=tabular_num_features_size,\n",
    "            tabular_num_features_ffn_hidden_size=tabular_num_features_ffn_hidden_size,\n",
    "            tabular_num_features_ffn_output_dim=tabular_num_features_ffn_output_dim,\n",
    "            time_series_cat_features_size=time_series_cat_features_size,\n",
    "            time_series_cat_features_possible_nums=time_series_cat_features_possible_nums,\n",
    "            time_series_cat_features_embeddings_dim=time_series_cat_features_embeddings_dim,\n",
    "            time_series_size=enc_seq_len\n",
    "        )\n",
    "\n",
    "        self.decoder_embedding_layer = EmbeddingLayer(\n",
    "            tabular_cat_features_size=tabular_cat_features_size,\n",
    "            tabular_cat_features_possible_nums=tabular_cat_features_possible_nums,\n",
    "            tabular_cat_features_embeddings_dim=tabular_cat_features_embeddings_dim,\n",
    "            tabular_num_features_size=tabular_num_features_size,\n",
    "            tabular_num_features_ffn_hidden_size=tabular_num_features_ffn_hidden_size,\n",
    "            tabular_num_features_ffn_output_dim=tabular_num_features_ffn_output_dim,\n",
    "            time_series_cat_features_size=time_series_cat_features_size,\n",
    "            time_series_cat_features_possible_nums=time_series_cat_features_possible_nums,\n",
    "            time_series_cat_features_embeddings_dim=time_series_cat_features_embeddings_dim,\n",
    "            time_series_size=dec_seq_len\n",
    "        )\n",
    "\n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val,\n",
    "            out_features=num_predicted_features\n",
    "        )\n",
    "\n",
    "        # The encoder layer used in the paper is identical to the one used by\n",
    "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers,\n",
    "            norm=None\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerDecoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers,\n",
    "            norm=None\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                tabular_categorical_features,\n",
    "                tabular_numerical_features,\n",
    "                src_time_series_categorical_features,\n",
    "                src_time_series_numerical_features,\n",
    "                trg_time_series_categorical_features,\n",
    "                trg_time_series_numerical_features,\n",
    "                src_mask: Tensor = None,\n",
    "                tgt_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "\n",
    "        Args:\n",
    "\n",
    "            src: the encoder's output sequence. Shape: (S,E) for unbatched input,\n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if\n",
    "                 batch_first=True, where S is the source sequence length,\n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "\n",
    "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input,\n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if\n",
    "                 batch_first=True, where T is the target sequence length,\n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "\n",
    "            src_mask: the mask for the src sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        # print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_embedding_layer(\n",
    "            tabular_cat_features=tabular_categorical_features,\n",
    "            tabular_num_features=tabular_numerical_features,\n",
    "            time_series_cat_features=src_time_series_categorical_features,\n",
    "            time_series_num_features=src_time_series_numerical_features\n",
    "        )  # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        # print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        # Masking is only needed in the encoder if input sequences are padded\n",
    "        # which they are not in this time series use case, because all my\n",
    "        # input sequences are naturally of the same length.\n",
    "        # (https://github.com/huggingface/transformers/issues/4083)\n",
    "        src = self.encoder(  # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "        )\n",
    "        # print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_embedding_layer(\n",
    "            tabular_cat_features=tabular_categorical_features,\n",
    "            tabular_num_features=tabular_numerical_features,\n",
    "            time_series_cat_features=trg_time_series_categorical_features,\n",
    "            time_series_num_features=trg_time_series_numerical_features\n",
    "        )  # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        # print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        # if src_mask is not None:\n",
    "        # print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        # if tgt_mask is not None:\n",
    "        # print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "        )\n",
    "\n",
    "        # print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output)  # shape [batch_size, target seq len]\n",
    "        # print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ad7d11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0773ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabular_categorical_features.shape = torch.Size([256, 2])\n",
      "tabular_numerical_features.shape = torch.Size([256, 1])\n",
      "src_time_series_categorical_features.shape = torch.Size([256, 10])\n",
      "src_time_series_numerical_features.shape = torch.Size([256, 10, 6])\n",
      "trg_time_series_categorical_features.shape = torch.Size([256, 4])\n",
      "trg_time_series_numerical_features.shape = torch.Size([256, 4, 6])\n",
      "trg_y.shape = torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    print(f\"tabular_categorical_features.shape = {data['tabular_categorical_features'].shape}\")\n",
    "    print(f\"tabular_numerical_features.shape = {data['tabular_numerical_features'].shape}\")\n",
    "    print(f\"src_time_series_categorical_features.shape = {data['src_time_series_categorical_features'].shape}\")\n",
    "    print(f\"src_time_series_numerical_features.shape = {data['src_time_series_numerical_features'].shape}\")\n",
    "    print(f\"trg_time_series_categorical_features.shape = {data['trg_time_series_categorical_features'].shape}\")\n",
    "    print(f\"trg_time_series_numerical_features.shape = {data['trg_time_series_numerical_features'].shape}\")\n",
    "    print(f\"trg_y.shape = {data['trg_y'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5a9877d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesTransformer(\n",
    "    tabular_cat_features_size=2,\n",
    "    tabular_cat_features_possible_nums=IntTensor([45, 81]),\n",
    "    tabular_cat_features_embeddings_dim=128,\n",
    "    tabular_num_features_size=1,\n",
    "    tabular_num_features_ffn_hidden_size=512,\n",
    "    tabular_num_features_ffn_output_dim=128,\n",
    "    time_series_cat_features_size=1,\n",
    "    time_series_cat_features_possible_nums=IntTensor([2]),\n",
    "    time_series_cat_features_embeddings_dim=122,\n",
    "\n",
    "    enc_seq_len=10,  # 10\n",
    "\n",
    "    dec_seq_len = 4,\n",
    "    batch_first = True,\n",
    "    out_seq_len = 1,\n",
    "    dim_val = 512,\n",
    "    n_encoder_layers = 4,\n",
    "    n_decoder_layers = 4,\n",
    "    n_heads = 8,\n",
    "    dropout_encoder = 0.2,\n",
    "    dropout_decoder = 0.2,\n",
    "    dropout_pos_enc = 0.1,\n",
    "    dim_feedforward_encoder = 2048,\n",
    "    dim_feedforward_decoder = 2048,\n",
    "    num_predicted_features = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "01116d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "                tabular_categorical_features=data['tabular_categorical_features'],\n",
    "                tabular_numerical_features=data['tabular_numerical_features'],\n",
    "                src_time_series_categorical_features=data['src_time_series_categorical_features'],\n",
    "                src_time_series_numerical_features=data['src_time_series_numerical_features'],\n",
    "                trg_time_series_categorical_features=data['trg_time_series_categorical_features'],\n",
    "                trg_time_series_numerical_features=data['trg_time_series_numerical_features'],\n",
    "                src_mask = None,\n",
    "                tgt_mask = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8f5a8fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6ae08563",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "lr = 0.00001  # learning rate\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "mae_metric = metrics.point.MAE()\n",
    "\n",
    "def train(model: nn.Module, epoch: int) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_dataset) // batch_size\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        output = model(\n",
    "            tabular_categorical_features=data['tabular_categorical_features'],\n",
    "            tabular_numerical_features=data['tabular_numerical_features'],\n",
    "            src_time_series_categorical_features=data['src_time_series_categorical_features'],\n",
    "            src_time_series_numerical_features=data['src_time_series_numerical_features'],\n",
    "            trg_time_series_categorical_features=data['trg_time_series_categorical_features'],\n",
    "            trg_time_series_numerical_features=data['trg_time_series_numerical_features'],\n",
    "            src_mask = None,\n",
    "            tgt_mask = None\n",
    "        )\n",
    "        \n",
    "        loss = criterion(output, data['trg_y'])\n",
    "        mae_value = mae_metric.loss(output, data['trg_y'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {i:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss}')\n",
    "            print(mae_value.mean())\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_dataloader) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(eval_dataloader):\n",
    "    \n",
    "            output = model(\n",
    "                tabular_categorical_features=data['tabular_categorical_features'],\n",
    "                tabular_numerical_features=data['tabular_numerical_features'],\n",
    "                src_time_series_categorical_features=data['src_time_series_categorical_features'],\n",
    "                src_time_series_numerical_features=data['src_time_series_numerical_features'],\n",
    "                trg_time_series_categorical_features=data['trg_time_series_categorical_features'],\n",
    "                trg_time_series_numerical_features=data['trg_time_series_numerical_features'],\n",
    "                src_mask = None,\n",
    "                tgt_mask = None\n",
    "            )\n",
    "\n",
    "            total_loss += criterion(output, data['trg_y']).item()\n",
    "    return total_loss / (len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1539fc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7723, 0.7696, 0.8235, 0.9412])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['trg_time_series_numerical_features'][:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88424490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8583])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['trg_y'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "50794b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL LOSS: 0.5729519707476435\n",
      "TEST LOSS: 0.6792062918345133\n",
      "| epoch  13 |   200/  979 batches | lr 0.00 | ms/batch 240.54 | loss 0.3318003011494875\n",
      "tensor(0.2876, grad_fn=<MeanBackward0>)\n",
      "| epoch  13 |   400/  979 batches | lr 0.00 | ms/batch 239.16 | loss 0.27071241594851014\n",
      "tensor(0.2693, grad_fn=<MeanBackward0>)\n",
      "| epoch  13 |   600/  979 batches | lr 0.00 | ms/batch 245.67 | loss 0.26369596339762214\n",
      "tensor(0.2727, grad_fn=<MeanBackward0>)\n",
      "| epoch  13 |   800/  979 batches | lr 0.00 | ms/batch 233.73 | loss 0.23328933633863927\n",
      "tensor(0.1500, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.18666136767600944\n",
      "TEST LOSS: 0.198646346728007\n",
      "| epoch  14 |   200/  979 batches | lr 0.00 | ms/batch 238.13 | loss 0.1196696351096034\n",
      "tensor(0.1152, grad_fn=<MeanBackward0>)\n",
      "| epoch  14 |   400/  979 batches | lr 0.00 | ms/batch 233.30 | loss 0.1028487154468894\n",
      "tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "| epoch  14 |   600/  979 batches | lr 0.00 | ms/batch 233.44 | loss 0.09248600129038095\n",
      "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "| epoch  14 |   800/  979 batches | lr 0.00 | ms/batch 258.44 | loss 0.08866980161517858\n",
      "tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.14389493451253768\n",
      "TEST LOSS: 0.0752045710881551\n",
      "| epoch  15 |   200/  979 batches | lr 0.00 | ms/batch 233.04 | loss 0.08056941205635666\n",
      "tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "| epoch  15 |   400/  979 batches | lr 0.00 | ms/batch 230.20 | loss 0.07571663461625576\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "| epoch  15 |   600/  979 batches | lr 0.00 | ms/batch 241.96 | loss 0.07410655634477735\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "| epoch  15 |   800/  979 batches | lr 0.00 | ms/batch 237.04 | loss 0.07069440333172679\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.1387849285101598\n",
      "TEST LOSS: 0.040000987549622856\n",
      "| epoch  16 |   200/  979 batches | lr 0.00 | ms/batch 230.92 | loss 0.06816600868478417\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "| epoch  16 |   400/  979 batches | lr 0.00 | ms/batch 239.46 | loss 0.06632681116461754\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "| epoch  16 |   600/  979 batches | lr 0.00 | ms/batch 298.61 | loss 0.06464548299089073\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "| epoch  16 |   800/  979 batches | lr 0.00 | ms/batch 285.81 | loss 0.06439661337062716\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.14370827493400662\n",
      "TEST LOSS: 0.08871745069821675\n",
      "| epoch  17 |   200/  979 batches | lr 0.00 | ms/batch 225.80 | loss 0.0623242137208581\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "| epoch  17 |   400/  979 batches | lr 0.00 | ms/batch 235.70 | loss 0.06079592254012823\n",
      "tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "| epoch  17 |   600/  979 batches | lr 0.00 | ms/batch 258.62 | loss 0.059348043892532586\n",
      "tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "| epoch  17 |   800/  979 batches | lr 0.00 | ms/batch 237.29 | loss 0.05875344041734934\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.13303618932977043\n",
      "TEST LOSS: 0.07287829245130222\n",
      "| epoch  18 |   200/  979 batches | lr 0.00 | ms/batch 245.09 | loss 0.05748067852109671\n",
      "tensor(0.0456, grad_fn=<MeanBackward0>)\n",
      "| epoch  18 |   400/  979 batches | lr 0.00 | ms/batch 248.95 | loss 0.05711766930297017\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "| epoch  18 |   600/  979 batches | lr 0.00 | ms/batch 282.16 | loss 0.056184535827487704\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "| epoch  18 |   800/  979 batches | lr 0.00 | ms/batch 253.96 | loss 0.05504951976239681\n",
      "tensor(0.0475, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.12800256601689053\n",
      "TEST LOSS: 0.09394055604934692\n",
      "| epoch  19 |   200/  979 batches | lr 0.00 | ms/batch 251.83 | loss 0.05573304414749145\n",
      "tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "| epoch  19 |   400/  979 batches | lr 0.00 | ms/batch 264.71 | loss 0.053398967422544956\n",
      "tensor(0.0482, grad_fn=<MeanBackward0>)\n",
      "| epoch  19 |   600/  979 batches | lr 0.00 | ms/batch 254.32 | loss 0.054535554815083744\n",
      "tensor(0.0459, grad_fn=<MeanBackward0>)\n",
      "| epoch  19 |   800/  979 batches | lr 0.00 | ms/batch 257.36 | loss 0.052970837000757456\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.10843858641623719\n",
      "TEST LOSS: 0.04289622108141581\n",
      "| epoch  20 |   200/  979 batches | lr 0.00 | ms/batch 230.25 | loss 0.052278830092400315\n",
      "tensor(0.0495, grad_fn=<MeanBackward0>)\n",
      "| epoch  20 |   400/  979 batches | lr 0.00 | ms/batch 234.13 | loss 0.05192419828847051\n",
      "tensor(0.0473, grad_fn=<MeanBackward0>)\n",
      "| epoch  20 |   600/  979 batches | lr 0.00 | ms/batch 238.76 | loss 0.0518120414018631\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "| epoch  20 |   800/  979 batches | lr 0.00 | ms/batch 230.09 | loss 0.05146149687469006\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.11120348988226586\n",
      "TEST LOSS: 0.07579048971335094\n",
      "| epoch  21 |   200/  979 batches | lr 0.00 | ms/batch 212.54 | loss 0.050024828650057314\n",
      "tensor(0.0492, grad_fn=<MeanBackward0>)\n",
      "| epoch  21 |   400/  979 batches | lr 0.00 | ms/batch 214.79 | loss 0.05024260852485895\n",
      "tensor(0.0500, grad_fn=<MeanBackward0>)\n",
      "| epoch  21 |   600/  979 batches | lr 0.00 | ms/batch 212.52 | loss 0.04917150914669037\n",
      "tensor(0.0437, grad_fn=<MeanBackward0>)\n",
      "| epoch  21 |   800/  979 batches | lr 0.00 | ms/batch 208.26 | loss 0.05017912570387125\n",
      "tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.10191583738553743\n",
      "TEST LOSS: 0.06242602442701658\n",
      "| epoch  22 |   200/  979 batches | lr 0.00 | ms/batch 224.45 | loss 0.0490885679423809\n",
      "tensor(0.0486, grad_fn=<MeanBackward0>)\n",
      "| epoch  22 |   400/  979 batches | lr 0.00 | ms/batch 224.39 | loss 0.04859685506671667\n",
      "tensor(0.0494, grad_fn=<MeanBackward0>)\n",
      "| epoch  22 |   600/  979 batches | lr 0.00 | ms/batch 222.05 | loss 0.04810683131217956\n",
      "tensor(0.0420, grad_fn=<MeanBackward0>)\n",
      "| epoch  22 |   800/  979 batches | lr 0.00 | ms/batch 218.08 | loss 0.04849018208682537\n",
      "tensor(0.0460, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.09614278636644223\n",
      "TEST LOSS: 0.04227405538161596\n",
      "| epoch  23 |   200/  979 batches | lr 0.00 | ms/batch 218.82 | loss 0.047949844989925626\n",
      "tensor(0.0423, grad_fn=<MeanBackward0>)\n",
      "| epoch  23 |   400/  979 batches | lr 0.00 | ms/batch 213.17 | loss 0.04732756767421961\n",
      "tensor(0.0505, grad_fn=<MeanBackward0>)\n",
      "| epoch  23 |   600/  979 batches | lr 0.00 | ms/batch 224.20 | loss 0.046674140579998497\n",
      "tensor(0.0463, grad_fn=<MeanBackward0>)\n",
      "| epoch  23 |   800/  979 batches | lr 0.00 | ms/batch 224.75 | loss 0.04692962011322379\n",
      "tensor(0.0385, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.09987429835679341\n",
      "TEST LOSS: 0.06506228571136792\n",
      "| epoch  24 |   200/  979 batches | lr 0.00 | ms/batch 208.41 | loss 0.04633410198614001\n",
      "tensor(0.0423, grad_fn=<MeanBackward0>)\n",
      "| epoch  24 |   400/  979 batches | lr 0.00 | ms/batch 205.74 | loss 0.04628382507711649\n",
      "tensor(0.0409, grad_fn=<MeanBackward0>)\n",
      "| epoch  24 |   600/  979 batches | lr 0.00 | ms/batch 210.19 | loss 0.04606429565697909\n",
      "tensor(0.0477, grad_fn=<MeanBackward0>)\n",
      "| epoch  24 |   800/  979 batches | lr 0.00 | ms/batch 207.96 | loss 0.04548621816560626\n",
      "tensor(0.0396, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.1014968968836442\n",
      "TEST LOSS: 0.05610718702276548\n",
      "| epoch  25 |   200/  979 batches | lr 0.00 | ms/batch 215.63 | loss 0.04572582958266139\n",
      "tensor(0.0394, grad_fn=<MeanBackward0>)\n",
      "| epoch  25 |   400/  979 batches | lr 0.00 | ms/batch 206.93 | loss 0.04521006230264902\n",
      "tensor(0.0436, grad_fn=<MeanBackward0>)\n",
      "| epoch  25 |   600/  979 batches | lr 0.00 | ms/batch 207.79 | loss 0.045775422938168046\n",
      "tensor(0.0391, grad_fn=<MeanBackward0>)\n",
      "| epoch  25 |   800/  979 batches | lr 0.00 | ms/batch 217.52 | loss 0.04477753933519125\n",
      "tensor(0.0408, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08957371047120884\n",
      "TEST LOSS: 0.044039794554313026\n",
      "| epoch  26 |   200/  979 batches | lr 0.00 | ms/batch 209.26 | loss 0.04553890625014901\n",
      "tensor(0.0449, grad_fn=<MeanBackward0>)\n",
      "| epoch  26 |   400/  979 batches | lr 0.00 | ms/batch 207.99 | loss 0.044089892115443945\n",
      "tensor(0.0484, grad_fn=<MeanBackward0>)\n",
      "| epoch  26 |   600/  979 batches | lr 0.00 | ms/batch 214.35 | loss 0.04367219187319279\n",
      "tensor(0.0430, grad_fn=<MeanBackward0>)\n",
      "| epoch  26 |   800/  979 batches | lr 0.00 | ms/batch 208.95 | loss 0.04389794269576669\n",
      "tensor(0.0463, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.0938223923550793\n",
      "TEST LOSS: 0.058459135393301644\n",
      "| epoch  27 |   200/  979 batches | lr 0.00 | ms/batch 215.48 | loss 0.04387504441663623\n",
      "tensor(0.0468, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  27 |   400/  979 batches | lr 0.00 | ms/batch 242.63 | loss 0.04312114791944623\n",
      "tensor(0.0450, grad_fn=<MeanBackward0>)\n",
      "| epoch  27 |   600/  979 batches | lr 0.00 | ms/batch 250.07 | loss 0.04397285560145974\n",
      "tensor(0.0389, grad_fn=<MeanBackward0>)\n",
      "| epoch  27 |   800/  979 batches | lr 0.00 | ms/batch 226.94 | loss 0.04318736616522074\n",
      "tensor(0.0434, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08750582730934664\n",
      "TEST LOSS: 0.04595216612021128\n",
      "| epoch  28 |   200/  979 batches | lr 0.00 | ms/batch 211.74 | loss 0.04344441372901201\n",
      "tensor(0.0518, grad_fn=<MeanBackward0>)\n",
      "| epoch  28 |   400/  979 batches | lr 0.00 | ms/batch 214.55 | loss 0.04303383383899927\n",
      "tensor(0.0368, grad_fn=<MeanBackward0>)\n",
      "| epoch  28 |   600/  979 batches | lr 0.00 | ms/batch 213.62 | loss 0.04254716398194432\n",
      "tensor(0.0365, grad_fn=<MeanBackward0>)\n",
      "| epoch  28 |   800/  979 batches | lr 0.00 | ms/batch 210.43 | loss 0.04277881540358067\n",
      "tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08946610808921007\n",
      "TEST LOSS: 0.059323432544867195\n",
      "| epoch  29 |   200/  979 batches | lr 0.00 | ms/batch 212.23 | loss 0.04297823609784245\n",
      "tensor(0.0486, grad_fn=<MeanBackward0>)\n",
      "| epoch  29 |   400/  979 batches | lr 0.00 | ms/batch 214.87 | loss 0.042151148784905675\n",
      "tensor(0.0386, grad_fn=<MeanBackward0>)\n",
      "| epoch  29 |   600/  979 batches | lr 0.00 | ms/batch 243.90 | loss 0.04242701819166541\n",
      "tensor(0.0367, grad_fn=<MeanBackward0>)\n",
      "| epoch  29 |   800/  979 batches | lr 0.00 | ms/batch 222.19 | loss 0.04211427699774504\n",
      "tensor(0.0434, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08459528469326306\n",
      "TEST LOSS: 0.05037733539938927\n",
      "| epoch  30 |   200/  979 batches | lr 0.00 | ms/batch 231.16 | loss 0.04265356972813606\n",
      "tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "| epoch  30 |   400/  979 batches | lr 0.00 | ms/batch 225.29 | loss 0.04182837462052703\n",
      "tensor(0.0416, grad_fn=<MeanBackward0>)\n",
      "| epoch  30 |   600/  979 batches | lr 0.00 | ms/batch 212.68 | loss 0.041954544801265\n",
      "tensor(0.0377, grad_fn=<MeanBackward0>)\n",
      "| epoch  30 |   800/  979 batches | lr 0.00 | ms/batch 209.71 | loss 0.04200974393635988\n",
      "tensor(0.0409, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08506828948763982\n",
      "TEST LOSS: 0.046097843597332634\n",
      "| epoch  31 |   200/  979 batches | lr 0.00 | ms/batch 224.58 | loss 0.04187743047252297\n",
      "tensor(0.0406, grad_fn=<MeanBackward0>)\n",
      "| epoch  31 |   400/  979 batches | lr 0.00 | ms/batch 216.87 | loss 0.04129830801859498\n",
      "tensor(0.0426, grad_fn=<MeanBackward0>)\n",
      "| epoch  31 |   600/  979 batches | lr 0.00 | ms/batch 212.68 | loss 0.041619887445122\n",
      "tensor(0.0465, grad_fn=<MeanBackward0>)\n",
      "| epoch  31 |   800/  979 batches | lr 0.00 | ms/batch 212.20 | loss 0.04088069209828973\n",
      "tensor(0.0372, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08406467923174607\n",
      "TEST LOSS: 0.048153712103764214\n",
      "| epoch  32 |   200/  979 batches | lr 0.00 | ms/batch 215.35 | loss 0.041440783962607385\n",
      "tensor(0.0385, grad_fn=<MeanBackward0>)\n",
      "| epoch  32 |   400/  979 batches | lr 0.00 | ms/batch 214.44 | loss 0.04052934432402253\n",
      "tensor(0.0380, grad_fn=<MeanBackward0>)\n",
      "| epoch  32 |   600/  979 batches | lr 0.00 | ms/batch 211.72 | loss 0.041384476590901616\n",
      "tensor(0.0386, grad_fn=<MeanBackward0>)\n",
      "| epoch  32 |   800/  979 batches | lr 0.00 | ms/batch 213.21 | loss 0.041378850564360616\n",
      "tensor(0.0434, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08116166814704615\n",
      "TEST LOSS: 0.04367733746767044\n",
      "| epoch  33 |   200/  979 batches | lr 0.00 | ms/batch 218.80 | loss 0.041263638995587826\n",
      "tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "| epoch  33 |   400/  979 batches | lr 0.00 | ms/batch 210.62 | loss 0.0405164435505867\n",
      "tensor(0.0449, grad_fn=<MeanBackward0>)\n",
      "| epoch  33 |   600/  979 batches | lr 0.00 | ms/batch 219.29 | loss 0.041284054275602104\n",
      "tensor(0.0382, grad_fn=<MeanBackward0>)\n",
      "| epoch  33 |   800/  979 batches | lr 0.00 | ms/batch 213.62 | loss 0.04017732525244355\n",
      "tensor(0.0389, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08745997620780775\n",
      "TEST LOSS: 0.06157560646533966\n",
      "| epoch  34 |   200/  979 batches | lr 0.00 | ms/batch 211.44 | loss 0.0409772483818233\n",
      "tensor(0.0428, grad_fn=<MeanBackward0>)\n",
      "| epoch  34 |   400/  979 batches | lr 0.00 | ms/batch 208.90 | loss 0.039971613585948945\n",
      "tensor(0.0478, grad_fn=<MeanBackward0>)\n",
      "| epoch  34 |   600/  979 batches | lr 0.00 | ms/batch 219.14 | loss 0.041100995521992444\n",
      "tensor(0.0439, grad_fn=<MeanBackward0>)\n",
      "| epoch  34 |   800/  979 batches | lr 0.00 | ms/batch 209.76 | loss 0.04033895926550031\n",
      "tensor(0.0410, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08801431937908834\n",
      "TEST LOSS: 0.060807490100463234\n",
      "| epoch  35 |   200/  979 batches | lr 0.00 | ms/batch 211.63 | loss 0.0402084450237453\n",
      "tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "| epoch  35 |   400/  979 batches | lr 0.00 | ms/batch 220.09 | loss 0.03980936726555228\n",
      "tensor(0.0400, grad_fn=<MeanBackward0>)\n",
      "| epoch  35 |   600/  979 batches | lr 0.00 | ms/batch 214.39 | loss 0.03984702859073877\n",
      "tensor(0.0385, grad_fn=<MeanBackward0>)\n",
      "| epoch  35 |   800/  979 batches | lr 0.00 | ms/batch 226.07 | loss 0.03974662823602557\n",
      "tensor(0.0393, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07672409181900185\n",
      "TEST LOSS: 0.04059989315768083\n",
      "| epoch  36 |   200/  979 batches | lr 0.00 | ms/batch 211.37 | loss 0.04051307484507561\n",
      "tensor(0.0388, grad_fn=<MeanBackward0>)\n",
      "| epoch  36 |   400/  979 batches | lr 0.00 | ms/batch 223.32 | loss 0.03924744358286261\n",
      "tensor(0.0357, grad_fn=<MeanBackward0>)\n",
      "| epoch  36 |   600/  979 batches | lr 0.00 | ms/batch 221.80 | loss 0.03938942635431886\n",
      "tensor(0.0367, grad_fn=<MeanBackward0>)\n",
      "| epoch  36 |   800/  979 batches | lr 0.00 | ms/batch 214.08 | loss 0.03937140280380845\n",
      "tensor(0.0353, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.0766522987671425\n",
      "TEST LOSS: 0.04045383632183075\n",
      "| epoch  37 |   200/  979 batches | lr 0.00 | ms/batch 243.00 | loss 0.039671191629022357\n",
      "tensor(0.0450, grad_fn=<MeanBackward0>)\n",
      "| epoch  37 |   400/  979 batches | lr 0.00 | ms/batch 236.38 | loss 0.039852721989154814\n",
      "tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "| epoch  37 |   600/  979 batches | lr 0.00 | ms/batch 228.60 | loss 0.03872810296714306\n",
      "tensor(0.0384, grad_fn=<MeanBackward0>)\n",
      "| epoch  37 |   800/  979 batches | lr 0.00 | ms/batch 220.32 | loss 0.03895128861069679\n",
      "tensor(0.0353, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08160937582056947\n",
      "TEST LOSS: 0.05498857796192169\n",
      "| epoch  38 |   200/  979 batches | lr 0.00 | ms/batch 214.31 | loss 0.039867624109610915\n",
      "tensor(0.0370, grad_fn=<MeanBackward0>)\n",
      "| epoch  38 |   400/  979 batches | lr 0.00 | ms/batch 211.83 | loss 0.03937523167580366\n",
      "tensor(0.0393, grad_fn=<MeanBackward0>)\n",
      "| epoch  38 |   600/  979 batches | lr 0.00 | ms/batch 217.18 | loss 0.03856882987543941\n",
      "tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "| epoch  38 |   800/  979 batches | lr 0.00 | ms/batch 217.27 | loss 0.03828696221113205\n",
      "tensor(0.0333, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08724345531931683\n",
      "TEST LOSS: 0.06804933771491051\n",
      "| epoch  39 |   200/  979 batches | lr 0.00 | ms/batch 214.30 | loss 0.03914388550445438\n",
      "tensor(0.0370, grad_fn=<MeanBackward0>)\n",
      "| epoch  39 |   400/  979 batches | lr 0.00 | ms/batch 215.17 | loss 0.038577402932569387\n",
      "tensor(0.0358, grad_fn=<MeanBackward0>)\n",
      "| epoch  39 |   600/  979 batches | lr 0.00 | ms/batch 213.37 | loss 0.038805089751258494\n",
      "tensor(0.0379, grad_fn=<MeanBackward0>)\n",
      "| epoch  39 |   800/  979 batches | lr 0.00 | ms/batch 215.30 | loss 0.03875533721409738\n",
      "tensor(0.0387, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08395989218646763\n",
      "TEST LOSS: 0.06155669937531153\n",
      "| epoch  40 |   200/  979 batches | lr 0.00 | ms/batch 215.81 | loss 0.03873991752974689\n",
      "tensor(0.0362, grad_fn=<MeanBackward0>)\n",
      "| epoch  40 |   400/  979 batches | lr 0.00 | ms/batch 214.47 | loss 0.03808797062374651\n",
      "tensor(0.0331, grad_fn=<MeanBackward0>)\n",
      "| epoch  40 |   600/  979 batches | lr 0.00 | ms/batch 215.40 | loss 0.03866346112452448\n",
      "tensor(0.0429, grad_fn=<MeanBackward0>)\n",
      "| epoch  40 |   800/  979 batches | lr 0.00 | ms/batch 215.97 | loss 0.038319231560453776\n",
      "tensor(0.0424, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.09497183647029604\n",
      "TEST LOSS: 0.08008512606223424\n",
      "| epoch  41 |   200/  979 batches | lr 0.00 | ms/batch 216.72 | loss 0.03852120610885322\n",
      "tensor(0.0330, grad_fn=<MeanBackward0>)\n",
      "| epoch  41 |   400/  979 batches | lr 0.00 | ms/batch 214.56 | loss 0.0379822588711977\n",
      "tensor(0.0439, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  41 |   600/  979 batches | lr 0.00 | ms/batch 215.36 | loss 0.03843190243467689\n",
      "tensor(0.0364, grad_fn=<MeanBackward0>)\n",
      "| epoch  41 |   800/  979 batches | lr 0.00 | ms/batch 217.41 | loss 0.03808434130623937\n",
      "tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.09331760162407635\n",
      "TEST LOSS: 0.0726784219344457\n",
      "| epoch  42 |   200/  979 batches | lr 0.00 | ms/batch 212.96 | loss 0.0384694479778409\n",
      "tensor(0.0406, grad_fn=<MeanBackward0>)\n",
      "| epoch  42 |   400/  979 batches | lr 0.00 | ms/batch 214.32 | loss 0.038208366874605416\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "| epoch  42 |   600/  979 batches | lr 0.00 | ms/batch 213.03 | loss 0.03769619431346655\n",
      "tensor(0.0402, grad_fn=<MeanBackward0>)\n",
      "| epoch  42 |   800/  979 batches | lr 0.00 | ms/batch 216.05 | loss 0.03781907557509839\n",
      "tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.0763164053733912\n",
      "TEST LOSS: 0.0491721418996652\n",
      "| epoch  43 |   200/  979 batches | lr 0.00 | ms/batch 217.98 | loss 0.03782084354199469\n",
      "tensor(0.0430, grad_fn=<MeanBackward0>)\n",
      "| epoch  43 |   400/  979 batches | lr 0.00 | ms/batch 213.03 | loss 0.03797952654771507\n",
      "tensor(0.0412, grad_fn=<MeanBackward0>)\n",
      "| epoch  43 |   600/  979 batches | lr 0.00 | ms/batch 212.57 | loss 0.03759013288654387\n",
      "tensor(0.0417, grad_fn=<MeanBackward0>)\n",
      "| epoch  43 |   800/  979 batches | lr 0.00 | ms/batch 214.49 | loss 0.037918676435947415\n",
      "tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07593605729059939\n",
      "TEST LOSS: 0.05041368554035822\n",
      "| epoch  44 |   200/  979 batches | lr 0.00 | ms/batch 214.90 | loss 0.037857346814125774\n",
      "tensor(0.0347, grad_fn=<MeanBackward0>)\n",
      "| epoch  44 |   400/  979 batches | lr 0.00 | ms/batch 213.46 | loss 0.037746378174051645\n",
      "tensor(0.0371, grad_fn=<MeanBackward0>)\n",
      "| epoch  44 |   600/  979 batches | lr 0.00 | ms/batch 218.33 | loss 0.038045439217239616\n",
      "tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "| epoch  44 |   800/  979 batches | lr 0.00 | ms/batch 212.97 | loss 0.03729289919137955\n",
      "tensor(0.0400, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08056536666546131\n",
      "TEST LOSS: 0.059067374716202416\n",
      "| epoch  45 |   200/  979 batches | lr 0.00 | ms/batch 215.37 | loss 0.03762044713832438\n",
      "tensor(0.0355, grad_fn=<MeanBackward0>)\n",
      "| epoch  45 |   400/  979 batches | lr 0.00 | ms/batch 216.33 | loss 0.03777248251251877\n",
      "tensor(0.0403, grad_fn=<MeanBackward0>)\n",
      "| epoch  45 |   600/  979 batches | lr 0.00 | ms/batch 218.15 | loss 0.03696730926632881\n",
      "tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "| epoch  45 |   800/  979 batches | lr 0.00 | ms/batch 210.65 | loss 0.03781936545856297\n",
      "tensor(0.0315, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08193276372332514\n",
      "TEST LOSS: 0.0558474970360597\n",
      "| epoch  46 |   200/  979 batches | lr 0.00 | ms/batch 215.27 | loss 0.03772733247838914\n",
      "tensor(0.0448, grad_fn=<MeanBackward0>)\n",
      "| epoch  46 |   400/  979 batches | lr 0.00 | ms/batch 211.86 | loss 0.03707410390488804\n",
      "tensor(0.0366, grad_fn=<MeanBackward0>)\n",
      "| epoch  46 |   600/  979 batches | lr 0.00 | ms/batch 215.30 | loss 0.03710263489745557\n",
      "tensor(0.0378, grad_fn=<MeanBackward0>)\n",
      "| epoch  46 |   800/  979 batches | lr 0.00 | ms/batch 212.68 | loss 0.037200841009616854\n",
      "tensor(0.0367, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07944538691109675\n",
      "TEST LOSS: 0.055591256668170295\n",
      "| epoch  47 |   200/  979 batches | lr 0.00 | ms/batch 214.89 | loss 0.037502153804525734\n",
      "tensor(0.0379, grad_fn=<MeanBackward0>)\n",
      "| epoch  47 |   400/  979 batches | lr 0.00 | ms/batch 211.75 | loss 0.03710141603834927\n",
      "tensor(0.0376, grad_fn=<MeanBackward0>)\n",
      "| epoch  47 |   600/  979 batches | lr 0.00 | ms/batch 213.17 | loss 0.03706558557227254\n",
      "tensor(0.0363, grad_fn=<MeanBackward0>)\n",
      "| epoch  47 |   800/  979 batches | lr 0.00 | ms/batch 222.54 | loss 0.03700272582471371\n",
      "tensor(0.0357, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07832720089543817\n",
      "TEST LOSS: 0.0526634119451046\n",
      "| epoch  48 |   200/  979 batches | lr 0.00 | ms/batch 214.55 | loss 0.037400864930823445\n",
      "tensor(0.0311, grad_fn=<MeanBackward0>)\n",
      "| epoch  48 |   400/  979 batches | lr 0.00 | ms/batch 214.20 | loss 0.03708587983623147\n",
      "tensor(0.0385, grad_fn=<MeanBackward0>)\n",
      "| epoch  48 |   600/  979 batches | lr 0.00 | ms/batch 216.42 | loss 0.03713888186961412\n",
      "tensor(0.0389, grad_fn=<MeanBackward0>)\n",
      "| epoch  48 |   800/  979 batches | lr 0.00 | ms/batch 211.91 | loss 0.03670973939821124\n",
      "tensor(0.0392, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.0734198413606436\n",
      "TEST LOSS: 0.04817110548416773\n",
      "| epoch  49 |   200/  979 batches | lr 0.00 | ms/batch 236.83 | loss 0.03670587514527142\n",
      "tensor(0.0302, grad_fn=<MeanBackward0>)\n",
      "| epoch  49 |   400/  979 batches | lr 0.00 | ms/batch 242.65 | loss 0.03670253897085786\n",
      "tensor(0.0352, grad_fn=<MeanBackward0>)\n",
      "| epoch  49 |   600/  979 batches | lr 0.00 | ms/batch 217.37 | loss 0.03692866663448512\n",
      "tensor(0.0380, grad_fn=<MeanBackward0>)\n",
      "| epoch  49 |   800/  979 batches | lr 0.00 | ms/batch 242.39 | loss 0.036768790353089574\n",
      "tensor(0.0350, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07052580709426315\n",
      "TEST LOSS: 0.04506038874387741\n",
      "| epoch  50 |   200/  979 batches | lr 0.00 | ms/batch 220.86 | loss 0.03698913572356105\n",
      "tensor(0.0394, grad_fn=<MeanBackward0>)\n",
      "| epoch  50 |   400/  979 batches | lr 0.00 | ms/batch 216.13 | loss 0.03645457070320845\n",
      "tensor(0.0353, grad_fn=<MeanBackward0>)\n",
      "| epoch  50 |   600/  979 batches | lr 0.00 | ms/batch 224.24 | loss 0.03650038003921509\n",
      "tensor(0.0336, grad_fn=<MeanBackward0>)\n",
      "| epoch  50 |   800/  979 batches | lr 0.00 | ms/batch 214.78 | loss 0.03654980801977217\n",
      "tensor(0.0446, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.078593113831033\n",
      "TEST LOSS: 0.06334589670101802\n",
      "| epoch  51 |   200/  979 batches | lr 0.00 | ms/batch 241.30 | loss 0.03647853480651975\n",
      "tensor(0.0381, grad_fn=<MeanBackward0>)\n",
      "| epoch  51 |   400/  979 batches | lr 0.00 | ms/batch 228.50 | loss 0.03673883250914514\n",
      "tensor(0.0378, grad_fn=<MeanBackward0>)\n",
      "| epoch  51 |   600/  979 batches | lr 0.00 | ms/batch 234.84 | loss 0.03606236920692027\n",
      "tensor(0.0307, grad_fn=<MeanBackward0>)\n",
      "| epoch  51 |   800/  979 batches | lr 0.00 | ms/batch 232.71 | loss 0.03659505102783442\n",
      "tensor(0.0370, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07458845172877326\n",
      "TEST LOSS: 0.05197650318344434\n",
      "| epoch  52 |   200/  979 batches | lr 0.00 | ms/batch 232.79 | loss 0.036046241968870164\n",
      "tensor(0.0381, grad_fn=<MeanBackward0>)\n",
      "| epoch  52 |   400/  979 batches | lr 0.00 | ms/batch 236.26 | loss 0.036498567769303915\n",
      "tensor(0.0379, grad_fn=<MeanBackward0>)\n",
      "| epoch  52 |   600/  979 batches | lr 0.00 | ms/batch 255.45 | loss 0.03625499155372381\n",
      "tensor(0.0384, grad_fn=<MeanBackward0>)\n",
      "| epoch  52 |   800/  979 batches | lr 0.00 | ms/batch 223.00 | loss 0.03620629583485425\n",
      "tensor(0.0337, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07325629883108695\n",
      "TEST LOSS: 0.053741140911976494\n",
      "| epoch  53 |   200/  979 batches | lr 0.00 | ms/batch 231.35 | loss 0.03598599039018154\n",
      "tensor(0.0306, grad_fn=<MeanBackward0>)\n",
      "| epoch  53 |   400/  979 batches | lr 0.00 | ms/batch 222.03 | loss 0.03623094525188208\n",
      "tensor(0.0384, grad_fn=<MeanBackward0>)\n",
      "| epoch  53 |   600/  979 batches | lr 0.00 | ms/batch 219.30 | loss 0.03597075700759888\n",
      "tensor(0.0380, grad_fn=<MeanBackward0>)\n",
      "| epoch  53 |   800/  979 batches | lr 0.00 | ms/batch 222.66 | loss 0.03637409320101142\n",
      "tensor(0.0392, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08891279238956107\n",
      "TEST LOSS: 0.07254152745008469\n",
      "| epoch  54 |   200/  979 batches | lr 0.00 | ms/batch 233.44 | loss 0.036034945799037814\n",
      "tensor(0.0348, grad_fn=<MeanBackward0>)\n",
      "| epoch  54 |   400/  979 batches | lr 0.00 | ms/batch 225.61 | loss 0.03595220069400966\n",
      "tensor(0.0370, grad_fn=<MeanBackward0>)\n",
      "| epoch  54 |   600/  979 batches | lr 0.00 | ms/batch 233.60 | loss 0.03601858490146696\n",
      "tensor(0.0310, grad_fn=<MeanBackward0>)\n",
      "| epoch  54 |   800/  979 batches | lr 0.00 | ms/batch 240.03 | loss 0.03564978421665728\n",
      "tensor(0.0417, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07061281542013759\n",
      "TEST LOSS: 0.04514786973595619\n",
      "| epoch  55 |   200/  979 batches | lr 0.00 | ms/batch 220.95 | loss 0.03602224342525005\n",
      "tensor(0.0346, grad_fn=<MeanBackward0>)\n",
      "| epoch  55 |   400/  979 batches | lr 0.00 | ms/batch 230.50 | loss 0.036295011742040514\n",
      "tensor(0.0350, grad_fn=<MeanBackward0>)\n",
      "| epoch  55 |   600/  979 batches | lr 0.00 | ms/batch 225.46 | loss 0.036093920366838575\n",
      "tensor(0.0348, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  55 |   800/  979 batches | lr 0.00 | ms/batch 234.15 | loss 0.036032288894057275\n",
      "tensor(0.0336, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.07309461963231578\n",
      "TEST LOSS: 0.054474695275227226\n",
      "| epoch  56 |   200/  979 batches | lr 0.00 | ms/batch 218.13 | loss 0.03628462525084615\n",
      "tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "| epoch  56 |   400/  979 batches | lr 0.00 | ms/batch 238.97 | loss 0.03519318397156894\n",
      "tensor(0.0337, grad_fn=<MeanBackward0>)\n",
      "| epoch  56 |   600/  979 batches | lr 0.00 | ms/batch 256.96 | loss 0.03573634250089526\n",
      "tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "| epoch  56 |   800/  979 batches | lr 0.00 | ms/batch 228.82 | loss 0.035713348025456074\n",
      "tensor(0.0381, grad_fn=<MeanBackward0>)\n",
      "VAL LOSS: 0.08184754898760216\n",
      "TEST LOSS: 0.0635523038605849\n",
      "| epoch  57 |   200/  979 batches | lr 0.00 | ms/batch 222.67 | loss 0.035840713009238245\n",
      "tensor(0.0352, grad_fn=<MeanBackward0>)\n",
      "| epoch  57 |   400/  979 batches | lr 0.00 | ms/batch 239.42 | loss 0.035603566439822314\n",
      "tensor(0.0371, grad_fn=<MeanBackward0>)\n",
      "| epoch  57 |   600/  979 batches | lr 0.00 | ms/batch 235.26 | loss 0.03563843163661659\n",
      "tensor(0.0411, grad_fn=<MeanBackward0>)\n",
      "| epoch  57 |   800/  979 batches | lr 0.00 | ms/batch 228.23 | loss 0.03544196057133377\n",
      "tensor(0.0310, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEST LOSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate(model,\u001b[38;5;250m \u001b[39mtest_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAL LOSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate(model,\u001b[38;5;250m \u001b[39mval_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEST LOSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate(model,\u001b[38;5;250m \u001b[39mtest_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[170], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m mae_value \u001b[38;5;241m=\u001b[39m mae_metric\u001b[38;5;241m.\u001b[39mloss(output, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg_y\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"VAL LOSS: {evaluate(model, val_dataloader)}\")\n",
    "print(f\"TEST LOSS: {evaluate(model, test_dataloader)}\")\n",
    "for i in range(13, 300):\n",
    "    train(model, i)\n",
    "    print(f\"VAL LOSS: {evaluate(model, val_dataloader)}\")\n",
    "    print(f\"TEST LOSS: {evaluate(model, test_dataloader)}\")\n",
    "    torch.save(model.state_dict(), f\"../models/model_epoch_mse_{i+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f1728e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04665118828415871"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f39daa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['src_time_series_numerical_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "16c34f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_time_series = data['src_time_series_numerical_features'][:, :, 0]\n",
    "trg_time_series = data['trg_time_series_numerical_features'][:, :, 0]\n",
    "trg_y = data['trg_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d5bfe9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_time_series[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c10cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_src_time_series = weekly_sales_quantile_transformer.inverse_transform(src_time_series[0].reshape(-1, 1))\n",
    "original_trg_time_series = weekly_sales_quantile_transformer.inverse_transform(trg_time_series[0].reshape(-1, 1))\n",
    "original_trg_y = weekly_sales_quantile_transformer.inverse_transform(trg_y[0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "62767400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_src_time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "45b97ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9189.26126075]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e50249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9797b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8280ab18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1]) 0.10273072868585587\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for i, data in enumerate(val_dataloader):\n",
    "    data['src_time_series_numerical_features']\n",
    "    \n",
    "    output = model(\n",
    "                tabular_categorical_features=data['tabular_categorical_features'],\n",
    "                tabular_numerical_features=data['tabular_numerical_features'],\n",
    "                src_time_series_categorical_features=data['src_time_series_categorical_features'],\n",
    "                src_time_series_numerical_features=data['src_time_series_numerical_features'],\n",
    "                trg_time_series_categorical_features=data['trg_time_series_categorical_features'],\n",
    "                trg_time_series_numerical_features=data['trg_time_series_numerical_features'],\n",
    "                src_mask = None,\n",
    "                tgt_mask = None\n",
    "            )\n",
    "\n",
    "    total_loss += criterion(output, data['trg_y']).item()\n",
    "    \n",
    "    print(output.shape, total_loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "582a9cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5cd7eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d3ba896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8015], grad_fn=<SelectBackward0>)\n",
      "tensor([0.9203])\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "print(output[i])\n",
    "print(data['trg_y'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "095df1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1027, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(output, data['trg_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e63a1f93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/astonuser/anaconda3/envs/transformer_time_series_forecast/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_output = weekly_sales_quantile_transformer.inverse_transform(output[0].detach().numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c5844fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18835.54]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15dcb097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2e4261730>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(original_src_time_series)\n",
    "plt.plot([9, 10, 11, 12], original_trg_time_series)\n",
    "plt.plot([13], original_trg_y, 'yo')\n",
    "plt.plot([13], original_output[0], 'ro')\n",
    "#'ro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4c5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ceb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c09b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d058a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fe4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec42ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674b4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a968a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667a0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "daeec93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_cat_features_size = 2\n",
    "tabular_cat_features_possible_nums = [45, 81]\n",
    "d_model = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2b05871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_cat_features_embeddings_table = torch.nn.ModuleList(\n",
    "            [torch.nn.Embedding(num_embeddings=tabular_cat_features_possible_nums[cat_feature_i],\n",
    "                                embedding_dim=d_model)\n",
    "             for cat_feature_i in range(tabular_cat_features_size)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "02e3c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_cat_embeddings = torch.sum(torch.stack([tabular_cat_features_embeddings_table[cat_feature_i](data['tabular_categorical_features'][:, cat_feature_i].long())\n",
    "             for cat_feature_i in range(data['tabular_categorical_features'].shape[1])], dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "da1646d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 32])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_cat_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fa314c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a783c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356a652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "id": "bf6f6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tabular_numerical_features'] = data['tabular_numerical_features'].reshape(1, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "id": "bb43d656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1])"
      ]
     },
     "execution_count": 1144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tabular_numerical_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "2460bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            output_size=32\n",
    "    ):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "fd04c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForwardLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "7ae516d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_num_features = ffn(data['tabular_numerical_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "53fbf501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32])"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_num_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "fc7147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_cat_features_size = 1\n",
    "time_series_cat_features_possible_nums = torch.IntTensor([2])\n",
    "time_series_cat_features_embeddings_dim = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "8bfd9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_cat_features_embeddings_table = nn.ModuleList(\n",
    "            [nn.Embedding(num_embeddings=time_series_cat_features_possible_nums[cat_feature_i], embedding_dim=time_series_cat_features_embeddings_dim)\n",
    "             for cat_feature_i in range(time_series_cat_features_size)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "id": "e1f6e5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 1246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['src_time_series_categorical_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "id": "b5f39914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['src_time_series_categorical_features'] = data['src_time_series_categorical_features'].reshape(10, 5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "id": "dac55a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = data['src_time_series_categorical_features'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "id": "839642f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 1])"
      ]
     },
     "execution_count": 1291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['src_time_series_categorical_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "id": "84e53827",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_cat_features=torch.cat([time_series_cat_features_embeddings_table[cat_feature_i](data['src_time_series_categorical_features'][:, :, cat_feature_i].long())\n",
    "             for cat_feature_i in range(data['src_time_series_categorical_features'].shape[-1])], dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "id": "813c54e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 26])"
      ]
     },
     "execution_count": 1293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_cat_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "6fb5520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_num_features = data['src_time_series_numerical_features'].reshape(10, 5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "ef4632f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 6])"
      ]
     },
     "execution_count": 1266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_num_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "id": "cc2497a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32])\n",
      "torch.Size([1, 5, 32])\n",
      "torch.Size([10, 5, 26])\n",
      "torch.Size([10, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(tabular_cat_embeddings.shape)\n",
    "print(tabular_num_features.shape)\n",
    "print(time_series_cat_features.shape)\n",
    "print(time_series_num_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "id": "22acaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_embeddings = torch.cat([tabular_cat_embeddings, tabular_num_features], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "id": "9019e775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 32])"
      ]
     },
     "execution_count": 1269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "a766c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(time_series_num_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "id": "0a636231",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_embeddings = torch.cat([time_series_cat_features, time_series_num_features], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "id": "154517cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 32])"
      ]
     },
     "execution_count": 1271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "id": "6f4fe014",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_embedding_table = torch.nn.Embedding(num_embeddings=3 + 1, embedding_dim=32)\n",
    "positional_embedding_table = torch.nn.Embedding(num_embeddings=10 + 1, embedding_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "id": "80d08cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 32])"
      ]
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "94c0667c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 32])"
      ]
     },
     "execution_count": 1217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "id": "a28751c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4668,  0.3048,  0.2959, -0.3498,  1.1223, -2.4230, -0.3287, -0.1352,\n",
       "         -1.5053,  0.3417,  0.4774,  1.0340,  0.5035, -0.6018,  1.7864, -0.8862,\n",
       "          0.9633,  1.8013, -0.1006,  0.4067,  1.1611,  1.1968, -0.0766,  0.6571,\n",
       "          1.4259,  0.2372, -1.3344, -0.3572,  0.9219,  0.7024, -0.9807,  1.0322]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 1218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_embedding_table(torch.IntTensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "id": "12aacf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_embeddings += column_embedding_table(torch.IntTensor([0]))\n",
    "for step_i in range(len(time_series_embeddings)):\n",
    "    time_series_embeddings[step_i] += positional_embedding_table(torch.IntTensor([step_i + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "id": "818f6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_embeddings += positional_embedding_table(torch.IntTensor([0]))\n",
    "for column_i in range(len(tabular_embeddings)):\n",
    "    tabular_embeddings[column_i] += column_embedding_table(torch.IntTensor([column_i + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c6ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "4f0ff405",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_embeddings = time_series_embeddings.reshape(5, 10, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "id": "ea9d5c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 32])"
      ]
     },
     "execution_count": 1222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "id": "57c5f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_embeddings = tabular_embeddings.reshape(5, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "id": "63fc3a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 32])"
      ]
     },
     "execution_count": 1224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "id": "019711fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catt = torch.cat([tabular_embeddings, time_series_embeddings], dim=1).reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "id": "d06c703d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 416])"
      ]
     },
     "execution_count": 1250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52f764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
